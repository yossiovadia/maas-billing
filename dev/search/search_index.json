{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"MaaS Platform Documentation","text":"<p>Welcome to the Models-as-a-Service (MaaS) Platform documentation.</p> <p>The MaaS Platform enhances the model serving capabilities of Open Data Hub by adding a management layer for self-service access control, rate limiting, and tier-based subscriptions.</p> <p>Use this platform to streamline the deployment of your models, monitor usage, and effectively manage costs.</p>"},{"location":"#documentation-overview","title":"\ud83d\udcda Documentation Overview","text":""},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ul> <li>QuickStart Guide - Complete platform deployment instructions</li> <li>Architecture - Overview of the MaaS Platform architecture</li> </ul>"},{"location":"#configuration-management","title":"\u2699\ufe0f Configuration &amp; Management","text":"<ul> <li>Tier Management - Configuring subscription tiers and access control</li> <li>Model Setup - Setting up models for MaaS</li> <li>Self-Service Model Access - Managing model access and policies</li> </ul>"},{"location":"#advanced-administration","title":"\ud83d\udd27 Advanced Administration","text":"<ul> <li>Observability - Monitoring, metrics, and dashboards</li> </ul>"},{"location":"architecture/","title":"MaaS Platform Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p>The MaaS Platform is designed as a cloud-native, Kubernetes-based solution that provides policy-based access control, rate limiting, and tier-based subscriptions for AI model serving. The architecture follows microservices principles and leverages OpenShift/Kubernetes native components for scalability and reliability.</p>"},{"location":"architecture/#architecture","title":"Architecture","text":""},{"location":"architecture/#high-level-architecture","title":"\ud83c\udfd7\ufe0f High-Level Architecture","text":"<p>The MaaS Platform is an end-to-end solution that leverages Kuadrant (Red Hat Connectivity Link) and Open Data Hub (Red Hat OpenShift AI)'s Model Serving capabilities to provide a fully managed, scalable, and secure self-service platform for AI model serving.</p> <p>All requests flow through the maas-default-gateway and RHCL components, which then route requests based on the path:</p> <ul> <li><code>/maas-api/*</code> requests \u2192 MaaS API (token retrieval, validates OpenShift Token via RHCL)</li> <li>Inference requests (<code>/v1/models</code>, <code>/v1/chat/completions</code>) \u2192 Model Serving (validates Service Account Token via RHCL)</li> </ul> <pre><code>graph TB\n    subgraph \"User Layer\"\n        User[Users]\n    end\n\n    subgraph \"Gateway &amp; Policy Layer\"\n        GatewayAPI[\"maas-default-gateway&lt;br/&gt;All Traffic Entry Point\"]\n        AuthPolicy[\"&lt;b&gt;Auth Policy&lt;/b&gt;&lt;br/&gt;Authorino&lt;br/&gt;Token Validation\"]\n        RateLimit[\"&lt;b&gt;Rate Limiting&lt;/b&gt;&lt;br/&gt;Limitador&lt;br/&gt;Usage Quotas\"]\n    end\n\n    subgraph \"Token Management Path\"\n        MaaSAPI[\"MaaS API&lt;br/&gt;Token Retrieval\"]\n    end\n\n    subgraph \"Model Serving Path\"\n        PathInference[\"Inference Service\"]\n        ModelServing[\"RHOAI Model Serving\"]\n    end\n\n    User --&gt;|\"All Requests\"| GatewayAPI\n    GatewayAPI --&gt;|\"All Traffic\"| AuthPolicy\n\n    AuthPolicy --&gt;|\"/maas-api&lt;br/&gt;Auth Only\"| MaaSAPI\n    MaaSAPI --&gt;|\"Returns Token\"| User\n\n    AuthPolicy --&gt;|\"Inference Traffic&lt;br/&gt;Auth + Rate Limit\"| RateLimit\n    RateLimit --&gt; PathInference\n    PathInference --&gt; ModelServing\n    ModelServing --&gt;|\"Returns Response\"| User\n\n    style MaaSAPI fill:#1976d2,stroke:#333,stroke-width:2px,color:#fff\n    style GatewayAPI fill:#7b1fa2,stroke:#333,stroke-width:2px,color:#fff\n    style AuthPolicy fill:#f57c00,stroke:#333,stroke-width:2px,color:#fff\n    style RateLimit fill:#f57c00,stroke:#333,stroke-width:2px,color:#fff\n    style PathInference fill:#388e3c,stroke:#333,stroke-width:2px,color:#fff\n    style ModelServing fill:#388e3c,stroke:#333,stroke-width:2px,color:#fff</code></pre>"},{"location":"architecture/#architecture-details","title":"Architecture Details","text":"<p>The MaaS Platform architecture is designed to be modular and scalable. It is composed of the following components:</p> <ul> <li>maas-default-gateway: The single entry point for all traffic (both token requests and inference requests).</li> <li>RHCL (Red Hat Connectivity Link): The policy engine that handles authentication and authorization for all requests. Routes requests to appropriate backend based on path:</li> <li><code>/maas-api/*</code> \u2192 MaaS API (validates OpenShift tokens)</li> <li>Inference paths (<code>/v1/models</code>, <code>/v1/chat/completions</code>) \u2192 Model Serving (validates Service Account tokens)</li> <li>MaaS API: The central component for token generation and management, accessed via <code>/maas-api</code> path.</li> <li>Open Data Hub (Red Hat OpenShift AI): The model serving platform that handles inference requests.</li> </ul>"},{"location":"architecture/#detailed-component-architecture","title":"Detailed Component Architecture","text":""},{"location":"architecture/#maas-api-component-details","title":"MaaS API Component Details","text":"<p>The MaaS API provides a self-service platform for users to request tokens for their inference requests. All requests to the MaaS API pass through the <code>maas-default-gateway</code> where authentication is performed against the user's OpenShift token via the Auth Policy component. By leveraging Kubernetes native objects like ConfigMaps and ServiceAccounts, it offers model owners a simple way to configure access to their models based on a familiar group-based access control model.</p> <pre><code>graph TB\n    subgraph \"External Access\"\n        User[Users]\n        AdminUI[Admin/User UI]\n    end\n\n    subgraph \"Gateway &amp; Auth\"\n        Gateway[**maas-default-gateway**&lt;br/&gt;Entry Point]\n        AuthPolicy[**Auth Policy**&lt;br/&gt;Validates OpenShift Token]\n    end\n\n    subgraph \"MaaS API Service\"\n        API[**MaaS API**&lt;br/&gt;Go + Gin Framework]\n        TierMapping[**Tier Mapping Logic**]\n        TokenGen[**Service Account Token Generation**]\n    end\n\n    subgraph \"Configuration\"\n        ConfigMap[**ConfigMap**&lt;br/&gt;tier-to-group-mapping]\n        K8sGroups[**Kubernetes Groups**&lt;br/&gt;tier-free-users&lt;br/&gt;tier-premium-users&lt;br/&gt;tier-enterprise-users]\n    end\n\n    subgraph \"free namespace\"\n        FreeSA1[**ServiceAccount**&lt;br/&gt;freeuser1-sa]\n        FreeSA2[**ServiceAccount**&lt;br/&gt;freeuser2-sa]\n    end\n\n    subgraph \"premium namespace\"\n        PremiumSA1[**ServiceAccount**&lt;br/&gt;prem-user1-sa]\n    end\n\n    subgraph \"enterprise namespace\"\n        EnterpriseSA1[**ServiceAccount**&lt;br/&gt;ent-user1-sa]\n    end\n\n    User --&gt;|\"Request with&lt;br/&gt;OpenShift Token\"| Gateway\n    AdminUI --&gt;|\"Request with&lt;br/&gt;OpenShift Token\"| Gateway\n    Gateway --&gt;|\"/maas-api path\"| AuthPolicy\n    AuthPolicy --&gt;|\"Authenticated Request\"| API\n\n    API --&gt; TierMapping\n    API --&gt; TokenGen\n\n    TierMapping --&gt; ConfigMap\n    ConfigMap --&gt;|Maps Groups to Tiers| K8sGroups\n    TokenGen --&gt; FreeSA1\n    TokenGen --&gt; FreeSA2\n    TokenGen --&gt; PremiumSA1\n    TokenGen --&gt; EnterpriseSA1\n\n    K8sGroups --&gt;|Group Membership| TierMapping\n\n    style API fill:#1976d2,stroke:#333,stroke-width:2px,color:#fff\n    style ConfigMap fill:#f57c00,stroke:#333,stroke-width:2px,color:#fff\n    style K8sGroups fill:#f57c00,stroke:#333,stroke-width:2px,color:#fff\n    style FreeSA1 fill:#388e3c,stroke:#333,stroke-width:2px,color:#fff\n    style FreeSA2 fill:#388e3c,stroke:#333,stroke-width:2px,color:#fff\n    style PremiumSA1 fill:#388e3c,stroke:#333,stroke-width:2px,color:#fff\n    style EnterpriseSA1 fill:#388e3c,stroke:#333,stroke-width:2px,color:#fff</code></pre> <p>Key Features:</p> <ul> <li>Tier-to-Group Mapping: Uses ConfigMap in the same namespace as MaaS API to map Kubernetes groups to tiers</li> <li>Configurable Tiers: Out of the box, the MaaS Platform comes with three default tiers: free, premium, and enterprise. These tiers are configurable and can be extended to support more tiers as needed.</li> <li>Service Account Tokens: Generates tokens for the appropriate tier's service account based on user's group membership</li> <li>Future Enhancements: Planned improvements for more sophisticated token management and the ability to integrate with external identity providers.</li> </ul>"},{"location":"architecture/#inference-service-component-details","title":"Inference Service Component Details","text":"<p>Once a user has obtained their token through the MaaS API, they can use it to make inference requests to the Gateway API. RHCL's Application Connectivity Policies then validate the token and enforce access control and rate limiting policies:</p> <pre><code>graph TB\n    subgraph \"Client Layer\"\n        Client[Client Applications&lt;br/&gt;with Service Account Token]\n    end\n\n    subgraph \"Gateway Layer\"\n        GatewayAPI[**maas-default-gateway**&lt;br/&gt;maas.CLUSTER_DOMAIN]\n        Envoy[**Envoy Proxy**]\n    end\n\n    subgraph \"RHCL Policy Engine\"\n        Kuadrant[**Kuadrant**&lt;br/&gt;Policy Attachment]\n        Authorino[**Authorino**&lt;br/&gt;Authentication Service]\n        Limitador[**Limitador**&lt;br/&gt;Rate Limiting Service]\n    end\n\n    subgraph \"Policy Components\"\n        AuthPolicy[**AuthPolicy**&lt;br/&gt;gateway-auth-policy]\n        RateLimitPolicy[**RateLimitPolicy**&lt;br/&gt;gateway-rate-limits]\n        TokenRateLimitPolicy[**TokenRateLimitPolicy**&lt;br/&gt;gateway-token-rate-limits]\n    end\n\n    subgraph \"Model Access Control\"\n        RBAC[**Kubernetes RBAC**&lt;br/&gt;Service Account Permissions]\n        LLMInferenceService[**LLMInferenceService**&lt;br/&gt;Model Access Control]\n    end\n\n    subgraph \"Model Serving\"\n        RHOAI[**RHOAI Platform**]\n        Models[**LLM Models**&lt;br/&gt;Qwen, Granite, Llama]\n    end\n\n    subgraph \"Observability\"\n        Prometheus[**Prometheus**&lt;br/&gt;Metrics Collection]\n    end\n\n    Client --&gt;|Inference Request + Service Account Token| GatewayAPI\n    GatewayAPI --&gt; Envoy\n\n    Envoy --&gt; Kuadrant\n    Kuadrant --&gt; Authorino\n    Kuadrant --&gt; Limitador\n\n    Authorino --&gt; AuthPolicy\n    Limitador --&gt; RateLimitPolicy\n    Limitador --&gt; TokenRateLimitPolicy\n\n    Envoy --&gt;|Check Model Access| RBAC\n    RBAC --&gt; LLMInferenceService\n    LLMInferenceService --&gt;|POST Permission Check| RHOAI\n    RHOAI --&gt; Models\n\n    Limitador --&gt;|Usage Metrics| Prometheus\n\n    style GatewayAPI fill:#7b1fa2,stroke:#333,stroke-width:2px,color:#fff\n    style Kuadrant fill:#f57c00,stroke:#333,stroke-width:2px,color:#fff\n    style Authorino fill:#f57c00,stroke:#333,stroke-width:2px,color:#fff\n    style Limitador fill:#f57c00,stroke:#333,stroke-width:2px,color:#fff\n    style AuthPolicy fill:#d32f2f,stroke:#333,stroke-width:2px,color:#fff\n    style RateLimitPolicy fill:#d32f2f,stroke:#333,stroke-width:2px,color:#fff\n    style TokenRateLimitPolicy fill:#d32f2f,stroke:#333,stroke-width:2px,color:#fff\n    style RBAC fill:#d32f2f,stroke:#333,stroke-width:2px,color:#fff\n    style LLMInferenceService fill:#d32f2f,stroke:#333,stroke-width:2px,color:#fff\n    style RHOAI fill:#388e3c,stroke:#333,stroke-width:2px,color:#fff\n    style Models fill:#388e3c,stroke:#333,stroke-width:2px,color:#fff\n    style Prometheus fill:#1976d2,stroke:#333,stroke-width:2px,color:#fff</code></pre> <p>Policy Engine Flow:</p> <ol> <li>User Request: A user makes an inference request to the Gateway API with a valid token.</li> <li>Service Account Authentication: Authorino validates service account tokens using gateway-auth-policy</li> <li>Rate Limiting: Limitador enforces usage quotas per tier/user using gateway-rate-limits and gateway-token-rate-limits</li> <li>Model Access Control: RBAC checks if service account has POST access to the specific LLMInferenceService</li> <li>Request Forwarding: Only requests with proper model access are forwarded to RHOAI</li> <li>Metrics Collection: Limitador sends usage data to Prometheus for observability dashboards</li> </ol>"},{"location":"architecture/#component-flows","title":"\ud83d\udd04 Component Flows","text":""},{"location":"architecture/#1-token-retrieval-flow-maas-api","title":"1. Token Retrieval Flow (MaaS API)","text":"<p>The MaaS API generates service account tokens based on user group membership and tier configuration:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Gateway as Gateway API\n    participant Authorino\n    participant MaaS as MaaS API\n    participant TierMapper as Tier Mapper\n    participant K8s as Kubernetes API\n\n    User-&gt;&gt;Gateway: POST /maas-api/v1/tokens&lt;br/&gt;Authorization: Bearer {openshift-token}\n    Gateway-&gt;&gt;Authorino: Enforce MaaS API AuthPolicy\n    Authorino-&gt;&gt;K8s: TokenReview (validate OpenShift token)\n    K8s--&gt;&gt;Authorino: User identity (username, groups)\n    Authorino-&gt;&gt;Gateway: Authenticated\n    Gateway-&gt;&gt;MaaS: Forward request with user context\n\n    Note over MaaS,TierMapper: Determine User Tier\n    MaaS-&gt;&gt;TierMapper: GetTierForGroups(user.groups)\n    TierMapper-&gt;&gt;K8s: Get ConfigMap(tier-to-group-mapping)\n    K8s--&gt;&gt;TierMapper: Tier configuration\n    TierMapper--&gt;&gt;MaaS: User tier (e.g., \"premium\")\n\n    Note over MaaS,K8s: Ensure Tier Resources\n    MaaS-&gt;&gt;K8s: Create Namespace({instance}-tier-{tier}) if needed\n    MaaS-&gt;&gt;K8s: Create ServiceAccount({username-hash}) if needed\n\n    Note over MaaS,K8s: Generate Token\n    MaaS-&gt;&gt;K8s: CreateToken(namespace, SA name, TTL)\n    K8s--&gt;&gt;MaaS: TokenRequest with token and expiration\n\n    MaaS--&gt;&gt;User: {&lt;br/&gt;  \"token\": \"...\",&lt;br/&gt;  \"expiration\": \"4h\",&lt;br/&gt;  \"expiresAt\": 1234567890&lt;br/&gt;}</code></pre>"},{"location":"architecture/#3-model-inference-flow","title":"3. Model Inference Flow","text":"<p>The inference flow routes validated requests to RHOAI models:</p> <p>The Gateway API and RHCL components validate service account tokens and enforce policies:</p> <pre><code>sequenceDiagram\n    participant Client\n    participant GatewayAPI\n    participant Kuadrant\n    participant Authorino\n    participant Limitador\n    participant AuthPolicy\n    participant RateLimitPolicy\n    participant LLMInferenceService\n\n    Client-&gt;&gt;GatewayAPI: Inference Request + Service Account Token\n    GatewayAPI-&gt;&gt;Kuadrant: Applying Policies\n    Kuadrant-&gt;&gt;Authorino: Validate Service Account Token\n    Authorino-&gt;&gt;AuthPolicy: Check Token Validity\n    AuthPolicy--&gt;&gt;Authorino: Token Valid + Tier Info\n    Authorino--&gt;&gt;Kuadrant: Authentication Success\n    Kuadrant-&gt;&gt;Limitador: Check Rate Limits\n    Limitador-&gt;&gt;RateLimitPolicy: Apply Tier-based Limits\n    RateLimitPolicy--&gt;&gt;Limitador: Rate Limit Status\n    Limitador--&gt;&gt;Kuadrant: Rate Check Result\n    Kuadrant--&gt;&gt;GatewayAPI: Policy Decision (Allow/Deny)\n    GatewayAPI -&gt;&gt; LLMInferenceService: Forward Request\n    LLMInferenceService--&gt;&gt;Client: Response</code></pre>"},{"location":"quickstart/","title":"Installation Guide","text":"<p>This guide provides quickstart instructions for deploying the MaaS Platform infrastructure.</p> <p>Note</p> <p>For more detailed instructions, please refer to Installation under the Administrator Guide.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenShift cluster (4.19.9+) with kubectl/oc access<ul> <li>Recommended 16 vCPUs, 32GB RAM, 100GB storage</li> </ul> </li> <li>ODH/RHOAI requirements:<ul> <li>RHOAI 3.0 +</li> <li>ODH 3.0 +</li> </ul> </li> <li>RHCL requirements (Note: This can be installed automatically by the script below):<ul> <li>RHCL 1.2 +</li> </ul> </li> <li>Authorino TLS: Listener TLS must be enabled on Authorino (see Configure Authorino TLS)</li> <li>Cluster admin or equivalent permissions</li> <li>Required tools:<ul> <li><code>oc</code> (OpenShift CLI)</li> <li><code>kubectl</code></li> <li><code>jq</code></li> <li><code>kustomize</code> (v5.7.0+)</li> <li><code>gsed</code> (GNU sed) - macOS only: <code>brew install gnu-sed</code></li> </ul> </li> </ul>"},{"location":"quickstart/#configure-authorino-tls","title":"Configure Authorino TLS","text":"<p>Before deploying MaaS, Authorino's listener TLS must be enabled. This is a platform prerequisite for secure <code>LLMInferenceService</code> communication:</p> <ul> <li>Gateway \u2192 Authorino (Listener TLS): Enable TLS on Authorino's gRPC listener for incoming authentication requests</li> </ul> <p>For step-by-step commands, see TLS Configuration: Authorino TLS Configuration.</p> <p>Automated configuration</p> <p>The <code>deploy-rhoai-stable.sh</code> script automatically configures all remaining TLS settings after deployment, including Gateway TLS bootstrap and Authorino \u2192 maas-api outbound TLS.</p>"},{"location":"quickstart/#quick-start","title":"Quick Start","text":""},{"location":"quickstart/#automated-openshift-deployment-recommended","title":"Automated OpenShift Deployment (Recommended)","text":"<p>For OpenShift clusters, use the unified automated deployment script:</p> <pre><code>export MAAS_REF=\"main\"  # Use the latest release tag, or \"main\" for development\n\n# Deploy using RHOAI operator (default)\n./scripts/deploy.sh\n\n# Or deploy using ODH operator\n./scripts/deploy.sh --operator-type odh\n\n# Or deploy using kustomize\n./scripts/deploy.sh --deployment-mode kustomize\n</code></pre> <p>Using Release Tags</p> <p>The <code>MAAS_REF</code> environment variable should reference a release tag (e.g., <code>v1.0.0</code>) for production deployments. The release workflow automatically updates all <code>MAAS_REF=\"main\"</code> references in documentation and scripts to use the new release tag when a release is created. Use <code>\"main\"</code> only for development/testing.</p>"},{"location":"quickstart/#verify-deployment","title":"Verify Deployment","text":"<p>The deployment script creates the following core resources:</p> <ul> <li>Gateway: <code>maas-default-gateway</code> in <code>openshift-ingress</code> namespace</li> <li>HTTPRoutes: <code>maas-api-route</code> in the <code>redhat-ods-applications</code> namespace (deployed by operator)</li> <li>Policies:</li> <li><code>maas-api-auth-policy</code> (deployed by operator) - Protects MaaS API</li> <li><code>gateway-auth-policy</code> (deployed by script) - Protects Gateway/model inference</li> <li><code>TokenRateLimitPolicy</code>, <code>RateLimitPolicy</code> (deployed by script) - Usage limits</li> <li>MaaS API: Deployment and service in <code>redhat-ods-applications</code> namespace (deployed by operator)</li> <li>Operators: Cert-manager, LWS, Red Hat Connectivity Link and Red Hat OpenShift AI.</li> </ul> <p>Check deployment status:</p> <pre><code># Check all namespaces\nkubectl get ns | grep -E \"kuadrant-system|kserve|opendatahub|redhat-ods-applications|llm\"\n\n# Check Gateway status\nkubectl get gateway -n openshift-ingress maas-default-gateway\n\n# Check policies\nkubectl get authpolicy -A\nkubectl get tokenratelimitpolicy -A\nkubectl get ratelimitpolicy -A\n\n# Check MaaS API (deployed by operator in redhat-ods-applications)\nkubectl get pods -n redhat-ods-applications -l app.kubernetes.io/name=maas-api\nkubectl get svc -n redhat-ods-applications maas-api\n\n# Check Kuadrant operators\nkubectl get pods -n kuadrant-system\n\n# Check RHOAI/KServe\nkubectl get pods -n kserve\nkubectl get pods -n redhat-ods-applications\n</code></pre> <p>TLS Configuration</p> <p>TLS is enabled by default. See TLS Configuration for details.</p> <p>For detailed validation and troubleshooting, see the Validation Guide.</p>"},{"location":"quickstart/#model-setup","title":"Model Setup","text":"<p>Note</p> <p>At least one model must be deployed to validate the installation using the Validation Guide.</p>"},{"location":"quickstart/#deploy-sample-models","title":"Deploy Sample Models","text":""},{"location":"quickstart/#simulator-model-cpu","title":"Simulator Model (CPU)","text":"<pre><code>PROJECT_DIR=$(git rev-parse --show-toplevel)\nkustomize build ${PROJECT_DIR}/docs/samples/models/simulator/ | kubectl apply -f -\n</code></pre>"},{"location":"quickstart/#facebook-opt-125m-model-cpu","title":"Facebook OPT-125M Model (CPU)","text":"<pre><code>PROJECT_DIR=$(git rev-parse --show-toplevel)\nkustomize build ${PROJECT_DIR}/docs/samples/models/facebook-opt-125m-cpu/ | kubectl apply -f -\n</code></pre>"},{"location":"quickstart/#qwen3-model-gpu-required","title":"Qwen3 Model (GPU Required)","text":"<p>Warning</p> <p>This model requires GPU nodes with <code>nvidia.com/gpu</code> resources available in your cluster.</p> <pre><code>PROJECT_DIR=$(git rev-parse --show-toplevel)\nkustomize build ${PROJECT_DIR}/docs/samples/models/qwen3/ | kubectl apply -f -\n</code></pre>"},{"location":"quickstart/#verify-model-deployment","title":"Verify Model Deployment","text":"<pre><code># Check LLMInferenceService status\nkubectl get llminferenceservices -n llm\n\n# Check pods\nkubectl get pods -n llm\n</code></pre>"},{"location":"quickstart/#update-existing-models-optional","title":"Update Existing Models (Optional)","text":"<p>To update an existing model, modify the <code>LLMInferenceService</code> to use the newly created <code>maas-default-gateway</code> gateway.</p> <pre><code>kubectl patch llminferenceservice my-production-model -n llm --type='json' -p='[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/gateway/refs/-\",\n    \"value\": {\n      \"name\": \"maas-default-gateway\",\n      \"namespace\": \"openshift-ingress\"\n    }\n  }\n]'\n</code></pre> <pre><code>apiVersion: serving.kserve.io/v1alpha1\nkind: LLMInferenceService\nmetadata:\n  name: my-production-model\nspec:\n  gateway:\n    refs:\n      - name: maas-default-gateway\n        namespace: openshift-ingress\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>After installation, proceed to Validation to test and verify your deployment.</p>"},{"location":"advanced-administration/limitador-persistence/","title":"Persisting Limitador Metric Counts","text":"<p>By default, Limitador stores its rate-limiting counters in memory. This provides high performance but has a significant drawback: if a Limitador pod restarts, scales down, or is rescheduled, all hit counts are lost.</p> <p>For persistent, production-ready rate limiting where counts are maintained across pod lifecycles, you must configure Limitador to use an external Redis backend.</p> <p>Warning</p> <p>Production Considerations: The basic Redis setup script provided in this document is intended for local development and validation only. For production deployments, follow the official Red Hat documentation for proper Redis configuration and high availability.</p>"},{"location":"advanced-administration/limitador-persistence/#table-of-contents","title":"Table of Contents","text":"<p>. Requirements for Persistent Counts . Example Limitador CR Configuration . Local Validation Script . How to Validate Persistence . Related Documentation</p>"},{"location":"advanced-administration/limitador-persistence/#requirements-for-persistent-counts","title":"Requirements for Persistent Counts","text":"<p>To enable persistence, two conditions must be met:</p> <p>. A Running Redis Instance: A Redis instance must be deployed and network-accessible from within the Kubernetes cluster.</p> <p>. Limitador Custom Resource (CR) Configuration: The Limitador CR that manages your deployment must be updated to point to the running Redis instance by specifying the storage configuration in its spec.</p>"},{"location":"advanced-administration/limitador-persistence/#example-limitador-cr-configuration","title":"Example Limitador CR Configuration","text":"<p>To configure Limitador to use Redis for persistent storage, you need to:</p> <p>. Create a Kubernetes Secret containing the Redis connection URL:</p> <pre><code>kubectl create secret generic redis-config \\\n  --from-literal=URL=redis://redis-service.redis-limitador.svc:6379 \\\n  --namespace=&lt;your-limitador-namespace&gt;\n</code></pre> <p>. Update your Limitador CR to reference the secret:</p> <pre><code>apiVersion: limitador.kuadrant.io/v1alpha1\nkind: Limitador\nmetadata:\n  name: limitador\nspec:\n  storage:\n    redis:\n      configSecretRef:\n        name: redis-config\n</code></pre> <p>Edit your existing Limitador CR:</p> <pre><code>kubectl edit limitador &lt;your-instance-name&gt; -n &lt;your-limitador-namespace&gt;\n</code></pre> <p>For detailed, official instructions on production Redis setup, refer to the Red Hat documentation:</p> <ul> <li>Red Hat Connectivity Link - Configure Redis</li> </ul>"},{"location":"advanced-administration/limitador-persistence/#local-validation-script-basic-dev-only-redis","title":"Local Validation Script (Basic Dev-only Redis)","text":"<p>A basic Redis setup script is provided for local development and validation. This script deploys a non-production Redis instance.</p> <p>Script Location: <code>scripts/setup-redis.sh</code></p>"},{"location":"advanced-administration/limitador-persistence/#namespace-selection","title":"Namespace Selection","text":"<p>The script uses a simple namespace selection logic:</p> <ul> <li><code>NAMESPACE</code> environment variable (if set)</li> <li>Default: <code>redis-limitador</code> (created automatically if it doesn't exist)</li> </ul> <p>This opinionated default simplifies troubleshooting and ensures consistent deployments.</p>"},{"location":"advanced-administration/limitador-persistence/#usage","title":"Usage","text":"<pre><code># Make the script executable\nchmod +x scripts/setup-redis.sh\n\n# Run with default namespace (redis-limitador)\n./scripts/setup-redis.sh\n\n# Or override with environment variable\nNAMESPACE=my-namespace ./scripts/setup-redis.sh\n</code></pre> <p>The script will:</p> <ul> <li>Create the namespace if it doesn't exist (for default <code>redis-limitador</code> namespace)</li> <li>Deploy a Redis Deployment and Service</li> <li>Wait for Redis to be ready</li> <li>Output instructions for creating a Secret and configuring your Limitador CR</li> </ul> <p>Note</p> <p>Single Source of Truth: The script content is maintained only in <code>scripts/setup-redis.sh</code>. Any updates to the script are automatically reflected when users download and run it.</p>"},{"location":"advanced-administration/limitador-persistence/#how-to-validate-persistence","title":"How to Validate Persistence","text":"<p>. Run the script: <code>./scripts/setup-redis.sh</code></p> <p>This will deploy Redis to the <code>redis-limitador</code> namespace by default (or use your <code>NAMESPACE</code> env var).</p> <p>. Follow the output instructions to create the Secret and configure your Limitador CR with the Redis storage configuration.</p> <p>. Send traffic against a rate-limited route until you have a non-zero hit count.</p> <p>You can verify metrics in Prometheus:</p> <pre><code># Port-forward to Prometheus (adjust namespace as needed)\nkubectl port-forward -n monitoring svc/prometheus-k8s 9090:9091\n\n# Query for authorized_hits metric\n# Open http://localhost:9090 and search for: authorized_hits\n</code></pre> <p>. Find your Limitador pod:</p> <pre><code>kubectl get pods -l app=limitador\n</code></pre> <p>. Delete the pod to force a restart:</p> <pre><code>kubectl delete pod &lt;limitador-pod-name&gt;\n</code></pre> <p>. Wait for the new pod to become Running:</p> <pre><code>kubectl get pods -l app=limitador -w\n</code></pre> <p>. Send another request to the same route. You will see that the metric count continues from its previous value instead of resetting to 1.</p>"},{"location":"advanced-administration/limitador-persistence/#related-documentation","title":"Related Documentation","text":"<ul> <li>Red Hat Connectivity Link - Configure Redis - Official Red Hat documentation for production Redis setup</li> </ul>"},{"location":"advanced-administration/observability/","title":"Observability","text":"<p>This document covers the observability stack for the MaaS Platform, including metrics collection, monitoring, and visualization.</p> <p>Important</p> <p>User Workload Monitoring must be enabled in order to collect metrics.</p> <p>Add <code>enableUserWorkload: true</code> to the <code>cluster-monitoring-config</code> in the <code>openshift-monitoring</code> namespace</p>"},{"location":"advanced-administration/observability/#overview","title":"Overview","text":"<p>As part of Dev Preview, MaaS Platform includes a basic observability stack that provides insights into system performance, usage patterns, and operational health.</p> <p>Note</p> <p>The observability stack will be enhanced in future releases.</p> <p>The observability stack consists of:</p> <ul> <li>Limitador: Rate limiting service that exposes usage and rate-limit metrics (with labels from TelemetryPolicy)</li> <li>Authorino: Authentication/authorization service that exposes auth evaluation metrics (<code>auth_server_*</code>)</li> <li>Istio Telemetry: Adds <code>tier</code> to gateway latency metrics for per-tier latency (P50/P95/P99)</li> <li>vLLM / llm-d / Simulator: Expose inference metrics (TTFT, ITL, queue depth, token throughput, KV-cache usage); llm-d also exposes EPP routing metrics</li> <li>Prometheus: Metrics collection and storage (uses OpenShift platform Prometheus)</li> <li>ServiceMonitors: Deployed to configure Prometheus metric scraping</li> <li>Visualization: Grafana dashboards (see Grafana documentation)</li> </ul>"},{"location":"advanced-administration/observability/#component-metrics-status","title":"Component Metrics Status","text":"Component Exposes Metrics? Scraped into Prometheus? In Dashboards? Limitador Yes (<code>/metrics</code>) Yes (Kuadrant PodMonitor or MaaS ServiceMonitor) Yes \u2014 16 panels use <code>authorized_hits</code>, <code>authorized_calls</code>, <code>limited_calls</code>, <code>limitador_up</code> Authorino Yes (<code>/metrics</code> + <code>/server-metrics</code>) Yes \u2014 <code>/metrics</code> via Kuadrant operator; <code>/server-metrics</code> via MaaS <code>authorino-server-metrics</code> ServiceMonitor Yes \u2014 Auth Evaluation Latency (P50/P95/P99), Auth Success/Deny Rate, plus pod-up check Istio Gateway Yes (Envoy <code>/stats/prometheus</code>) Yes (<code>istio-gateway-metrics</code> ServiceMonitor) Yes \u2014 latency histograms, request counts, error rates maas-api No \u2014 returns 404 on <code>/metrics</code> No Only pod-up check via <code>kube_pod_status_phase</code> vLLM / llm-d / Simulator Yes (vLLM metrics on <code>/metrics</code> port 8000; llm-d EPP metrics on port 9090) Yes \u2014 vLLM metrics via <code>kserve-llm-models</code> ServiceMonitor; EPP metrics require separate scrape config Yes \u2014 TTFT, ITL, queue depth, latency, tokens, cache, prompt/generation ratio, queue wait time (EPP metrics not yet in MaaS dashboards) <p>maas-api Metrics Gap</p> <p>The maas-api Go service does not expose a <code>/metrics</code> endpoint. Metrics such as API key creation rate, token issuance rate, model discovery latency, and request handler durations are not available in Prometheus. Adding Prometheus instrumentation (e.g. <code>promhttp</code> handler + application-specific counters/histograms) to the Go service is a recommended future improvement.</p>"},{"location":"advanced-administration/observability/#installation","title":"Installation","text":"<p>The observability stack is defined in <code>deployment/base/observability/</code>. It includes:</p> Resource Purpose TelemetryPolicy (<code>telemetry-policy.yaml</code>) Adds <code>user</code>, <code>tier</code>, and <code>model</code> labels to Limitador metrics. The <code>model</code> label (from <code>responseBodyJSON</code>) is available on <code>authorized_hits</code>; <code>authorized_calls</code> and <code>limited_calls</code> carry <code>user</code> and <code>tier</code>. Istio Telemetry (<code>istio-telemetry.yaml</code>) Adds <code>tier</code> label to gateway latency (<code>istio_request_duration_milliseconds_bucket</code>) for per-tier P50/P95/P99. <p>Deploy observability (after Gateway and AuthPolicy are in place, so <code>X-MaaS-Tier</code> is injected):</p> <pre><code>./scripts/install-observability.sh [--namespace NAMESPACE]\n</code></pre> <p>When using the full deployment script, this is applied automatically:</p> <pre><code>./scripts/deploy.sh\n</code></pre> <p>Prerequisites</p> <ul> <li>Tools: <code>kubectl</code>, <code>kustomize</code>, <code>jq</code>, <code>yq</code> must be installed</li> <li>Cluster state: Gateway, AuthPolicy (gateway-auth-policy), and tier lookup must be deployed first. The AuthPolicy injects <code>X-MaaS-Tier</code>, which Istio Telemetry reads to label latency by tier. Without it, the <code>tier</code> label on gateway latency will be empty.</li> <li>Namespace: Use <code>--namespace</code> if your MaaS API is deployed to a namespace other than <code>maas-api</code> (e.g. <code>--namespace opendatahub</code>)</li> </ul> <p>Optional: To scrape the Istio gateway (Envoy) metrics, use the ServiceMonitor in <code>deployment/components/observability/monitors/</code> if your deployment includes that component.</p>"},{"location":"advanced-administration/observability/#metrics-collection","title":"Metrics Collection","text":""},{"location":"advanced-administration/observability/#limitador-metrics","title":"Limitador Metrics","text":"<p>Limitador exposes the following Prometheus metrics (verified against Limitador source code):</p>"},{"location":"advanced-administration/observability/#core-limitador-metrics","title":"Core Limitador Metrics","text":"Metric Type Labels Description <code>limitador_up</code> Gauge \u2014 Limitador is running (1 = up) <code>datastore_partitioned</code> Gauge \u2014 Limitador is partitioned from backing datastore (0 = healthy) <code>datastore_latency</code> Histogram \u2014 Latency to the underlying counter datastore"},{"location":"advanced-administration/observability/#maas-usage-metrics-limitador-telemetrypolicy","title":"MaaS Usage Metrics (Limitador + TelemetryPolicy)","text":"<p>When Kuadrant TelemetryPolicy and TokenRateLimitPolicy are applied, Limitador exposes these counters with custom labels injected by the wasm-shim from auth context and the model response body. These are the primary metrics for usage dashboards and chargeback:</p> Metric Type Labels Description <code>authorized_hits</code> Counter <code>user</code>, <code>tier</code>, <code>model</code>, <code>limitador_namespace</code> Total tokens consumed per request (from <code>usage.total_tokens</code> in the model response; input + output combined). The <code>model</code> label is extracted via <code>responseBodyJSON(\"/model\")</code>. <code>authorized_calls</code> Counter <code>user</code>, <code>tier</code>, <code>limitador_namespace</code> Requests allowed (not rate-limited). <code>limited_calls</code> Counter <code>user</code>, <code>tier</code>, <code>limitador_namespace</code> Requests denied due to token rate limits. <p><code>model</code> label availability</p> <p>The <code>model</code> label is currently available only on <code>authorized_hits</code>. The <code>authorized_calls</code> and <code>limited_calls</code> metrics carry <code>user</code> and <code>tier</code> labels but not <code>model</code>, due to how the wasm-shim constructs the CEL evaluation context for these counters. This is a known upstream limitation tracked for improvement in Kuadrant.</p> <p>Gateway latency is labeled by tier only via Istio Telemetry (see Per-Tier Latency Tracking); per-user latency is not exposed on the gateway histogram to keep cardinality bounded.</p>"},{"location":"advanced-administration/observability/#authorino-metrics","title":"Authorino Metrics","text":"<p>Authorino exposes metrics on two separate endpoints:</p> Endpoint Metrics Scraped? <code>/metrics</code> Controller-runtime (reconcile counts, workqueue depth) Yes (<code>authorino-operator-monitor</code>, provided by Kuadrant) <code>/server-metrics</code> Auth evaluation metrics (see below) Yes (<code>authorino-server-metrics</code>, deployed by MaaS <code>install-observability.sh</code>) <p>Auth server metrics (exposed on <code>/server-metrics</code>, port 8080):</p> Metric Type Labels Description <code>auth_server_authconfig_total</code> Counter <code>namespace</code>, <code>authconfig</code> Total AuthConfig evaluations <code>auth_server_authconfig_duration_seconds</code> Histogram <code>namespace</code>, <code>authconfig</code> Auth evaluation latency <code>auth_server_authconfig_response_status</code> Counter <code>namespace</code>, <code>authconfig</code>, <code>status</code> Auth response status per AuthConfig (OK, denied, etc.) <code>auth_server_response_status</code> Counter <code>status</code> Aggregate auth response status across all AuthConfigs <code>grpc_server_handled_total</code> Counter <code>grpc_method</code>, <code>grpc_code</code> gRPC requests handled <code>grpc_server_handling_seconds</code> Histogram <code>grpc_method</code> gRPC request latency <code>grpc_server_msg_received_total</code> Counter <code>grpc_method</code> gRPC messages received <code>grpc_server_msg_sent_total</code> Counter <code>grpc_method</code> gRPC messages sent <code>grpc_server_started_total</code> Counter <code>grpc_method</code> gRPC requests started <p>MaaS ServiceMonitor</p> <p>The Kuadrant-provided <code>authorino-operator-monitor</code> only scrapes <code>/metrics</code> (controller-runtime stats). MaaS deploys an additional <code>authorino-server-metrics</code> ServiceMonitor to scrape <code>/server-metrics</code> for auth evaluation metrics. This is deployed automatically by <code>install-observability.sh</code>.</p> <p>Lazily registered metrics</p> <p>Authorino upstream documents additional per-evaluator metrics (<code>auth_server_evaluator_total</code>, <code>auth_server_evaluator_duration_seconds</code>, <code>auth_server_evaluator_cancelled</code>, <code>auth_server_evaluator_denied</code>). These are lazily registered and only appear when specific evaluator types (e.g. OPA, HTTP authorization) are triggered. The MaaS AuthPolicy uses <code>kubernetesTokenReview</code>, which does not emit these metrics. They are not listed in the table above because they are not present in a standard MaaS deployment.</p>"},{"location":"advanced-administration/observability/#vllm-model-server-metrics","title":"vLLM / Model Server Metrics","text":"<p>MaaS supports three model serving backends that expose Prometheus metrics on <code>/metrics</code> (port 8000), scraped by the <code>kserve-llm-models</code> ServiceMonitor:</p> <ul> <li>vLLM (current stable) \u2014 full-featured LLM inference server</li> <li>llm-d \u2014 llm-d inference platform (runs vLLM as backend + EPP routing layer)</li> <li>llm-d-inference-sim (v0.7.1) \u2014 lightweight simulator for testing without GPUs</li> </ul> <p>Supported versions:</p> Backend Minimum Version Sample Manifests vLLM v0.7.x stable \u2014 llm-d v0.1.x \u2014 llm-d-inference-sim v0.7.1 <code>docs/samples/models/simulator/</code>"},{"location":"advanced-administration/observability/#vllm-metrics-port-8000","title":"vLLM Metrics (port 8000)","text":"<p>All three backends expose <code>vllm:</code>-prefixed metrics. The table below shows which metrics each backend provides.</p> Metric Type Simulator vLLM llm-d Description <code>vllm:num_requests_running</code> Gauge Y Y Y Requests currently being processed <code>vllm:num_requests_waiting</code> Gauge Y Y Y Requests queued waiting for processing <code>vllm:e2e_request_latency_seconds</code> Histogram Y Y Y End-to-end inference latency <code>vllm:time_to_first_token_seconds</code> Histogram Y Y Y Time to First Token (TTFT) <code>vllm:request_prompt_tokens</code> Histogram Y Y Y Per-request prompt token counts (<code>_sum</code> gives cumulative total) <code>vllm:request_generation_tokens</code> Histogram Y Y Y Per-request generation token counts (<code>_sum</code> gives cumulative total) <code>vllm:inter_token_latency_seconds</code> Histogram Y Y Y Inter-Token Latency (ITL) <code>vllm:kv_cache_usage_perc</code> Gauge Y Y Y KV-cache usage (0-1) <code>vllm:prompt_tokens_total</code> Counter Y Y Y Total prompt tokens processed <code>vllm:generation_tokens_total</code> Counter Y Y Y Total generation tokens processed <code>vllm:request_queue_time_seconds</code> Histogram \u2014 Y Y Time requests wait in queue before processing (vLLM/llm-d only) <code>vllm:request_success_total</code> Counter Y Y Y Successful requests (<code>_total</code> suffix added by prometheus_client) <code>vllm:request_prefill_time_seconds</code> Histogram Y Y Y Time spent in prefill (prompt processing) phase <code>vllm:request_decode_time_seconds</code> Histogram Y Y Y Time spent in decode (token generation) phase <code>vllm:request_inference_time_seconds</code> Histogram Y \u2014 \u2014 Total inference time (simulator-specific) <code>vllm:request_params_max_tokens</code> Histogram Y \u2014 \u2014 Distribution of <code>max_tokens</code> request parameter <code>vllm:max_num_generation_tokens</code> Histogram Y \u2014 \u2014 Max generation tokens per request <code>vllm:lora_requests_info</code> Gauge Y \u2014 \u2014 LoRA adapter request info <code>vllm:cache_config_info</code> Gauge Y \u2014 \u2014 Cache configuration info (simulator-specific) <code>vllm:time_per_output_token_seconds</code> Histogram Y \u2014 \u2014 Legacy ITL name (kept by simulator for backward compat; not used by dashboards) <p>Simulator metric alignment</p> <p>As of v0.7.1, the simulator fully aligns with current vLLM metric names (<code>kv_cache_usage_perc</code>, <code>inter_token_latency_seconds</code>, <code>prompt_tokens_total</code>, <code>generation_tokens_total</code>). Older simulator versions (v0.6.x) used different names (<code>gpu_cache_usage_perc</code>, <code>time_per_output_token_seconds</code>) and are no longer supported by MaaS dashboards. The simulator also exposes additional metrics not used by MaaS dashboards (e.g. <code>request_inference_time_seconds</code>, <code>request_params_max_tokens</code>).</p> <p>Lazily registered metrics</p> <p>Some vLLM/simulator metrics are lazily registered \u2014 they only appear in <code>/metrics</code> output after the first event that triggers them. For example, <code>request_queue_time_seconds</code> (on real vLLM) only appears after a request actually queues (when <code>max-num-seqs</code> is exceeded). Similarly, histogram counters like <code>e2e_request_latency_seconds</code> only appear after the first inference request completes. Dashboard panels will show \"No Data\" until sufficient traffic has been generated. This is normal Prometheus client behavior, not a configuration issue.</p> <p>Counter <code>_total</code> suffix</p> <p>vLLM code defines counters as <code>vllm:prompt_tokens</code> and <code>vllm:generation_tokens</code>, but the Python prometheus_client library appends <code>_total</code> when exposing metrics. The actual scraped metric names in Prometheus are <code>vllm:prompt_tokens_total</code> and <code>vllm:generation_tokens_total</code>. The llm-d official dashboard confirms this by using the <code>_total</code> form.</p>"},{"location":"advanced-administration/observability/#llm-d-epp-endpoint-picker-metrics","title":"llm-d EPP (Endpoint Picker) Metrics","text":"<p>When using llm-d, the inference gateway's Endpoint Picker (EPP) exposes additional routing and scheduling metrics on a separate port (9090). These are complementary to vLLM metrics and require a separate ServiceMonitor:</p> Metric Type Description <code>inference_model_request_total</code> Counter Total inference requests per model <code>inference_model_request_error_total</code> Counter Total errored requests per model <code>inference_model_request_duration_seconds</code> Histogram Request duration through the EPP <code>inference_model_input_tokens</code> Counter Input tokens routed per model <code>inference_model_output_tokens</code> Counter Output tokens routed per model <code>inference_model_running_requests</code> Gauge Currently running requests per model <code>inference_pool_average_kv_cache_utilization</code> Gauge Average KV-cache utilization across the pool <code>inference_pool_average_queue_size</code> Gauge Average queue size across the pool <code>inference_pool_ready_pods</code> Gauge Number of ready pods in the inference pool <p>EPP metrics not yet in MaaS dashboards</p> <p>EPP metrics are not currently scraped or visualized by MaaS. When deploying llm-d with the EPP, refer to the llm-d monitoring docs and the inference gateway dashboard for EPP-specific visualization.</p> <p>Input/Output Token Split</p> <p>vLLM metrics provide input vs output token breakdown per model (<code>vllm:prompt_tokens_total</code> / <code>vllm:generation_tokens_total</code> counters, or <code>vllm:request_prompt_tokens</code> / <code>vllm:request_generation_tokens</code> histograms). However, these do not carry <code>user</code> or <code>tier</code> labels. For per-user billing with input/output split, upstream changes to the Kuadrant wasm-shim are required (see Known Limitations).</p>"},{"location":"advanced-administration/observability/#dashboard-metric-queries","title":"Dashboard Metric Queries","text":"<p>Dashboard panels use histogram <code>_sum</code> as primary data source. All queries work across vLLM, llm-d, and llm-d-inference-sim v0.7.1:</p> Panel PromQL metric Tokens (1h) <code>request_prompt_tokens_sum</code> + <code>request_generation_tokens_sum</code> Token Throughput <code>rate(request_prompt_tokens_sum)</code>, <code>rate(request_generation_tokens_sum)</code> Prompt/Gen Ratio <code>rate(request_prompt_tokens_sum)</code> / total ITL <code>inter_token_latency_seconds_bucket</code> KV Cache <code>kv_cache_usage_perc</code> Queue Wait Time <code>request_queue_time_seconds_bucket</code> (vLLM/llm-d only) <p>See the vLLM metrics documentation for the full vLLM metric list and deprecation policy, and the llm-d monitoring documentation for llm-d-specific setup.</p>"},{"location":"advanced-administration/observability/#servicemonitor-configuration","title":"ServiceMonitor Configuration","text":"<p>ServiceMonitors are deployed by <code>install-observability.sh</code> to configure OpenShift's Prometheus to discover and scrape metrics from MaaS components.</p> <p>Automatically Deployed:</p> <ul> <li>Istio Gateway: Scrapes Envoy metrics from the MaaS gateway in <code>openshift-ingress</code> (deployed if the gateway exists)</li> <li>KServe LLM Models: Scrapes vLLM metrics from model pods in the <code>llm</code> namespace (deployed if the <code>llm</code> namespace exists)</li> </ul> <p>Conditionally Deployed (auto-detected by <code>install-observability.sh</code>):</p> <ul> <li>Limitador (<code>servicemonitor.yaml</code>): Scrapes rate limiting metrics from Limitador pods in <code>kuadrant-system</code>. Skipped when Kuadrant's own PodMonitor is already present. When Kuadrant CR has <code>spec.observability.enable: true</code>, the operator creates its own <code>kuadrant-limitador-monitor</code> PodMonitor that scrapes the same Limitador pod. Deploying both would cause duplicate metrics.</li> <li>Authorino Server Metrics (<code>authorino-server-metrics-servicemonitor.yaml</code>): Scrapes auth evaluation metrics from Authorino's <code>/server-metrics</code> endpoint in <code>kuadrant-system</code>. Skipped if a Kuadrant-provided monitor already scrapes <code>/server-metrics</code>. This collects <code>auth_server_authconfig_duration_seconds</code>, <code>auth_server_authconfig_response_status</code>, and other auth server metrics that are not scraped by the Kuadrant-provided <code>authorino-operator-monitor</code> (which only covers <code>/metrics</code> for controller-runtime stats).</li> </ul> <p>Already Provided by Kuadrant (when <code>observability.enable: true</code>):</p> <ul> <li>Limitador PodMonitor (<code>kuadrant-limitador-monitor</code>): Created by the Kuadrant operator</li> <li>Authorino Operator Monitor (<code>authorino-operator-monitor</code>): Scrapes Authorino controller metrics from <code>/metrics</code> only</li> </ul> <p>Authorino Metrics Coverage</p> <p>The Kuadrant-provided <code>authorino-operator-monitor</code> only scrapes <code>/metrics</code> (controller-runtime stats). The MaaS <code>authorino-server-metrics</code> ServiceMonitor supplements this by scraping <code>/server-metrics</code> for auth evaluation metrics (<code>auth_server_authconfig_duration_seconds</code>, <code>auth_server_authconfig_response_status</code>, etc.). The <code>install-observability.sh</code> script auto-detects whether a Kuadrant-provided monitor already scrapes <code>/server-metrics</code> and skips deploying the MaaS ServiceMonitor to avoid duplicates. See Authorino Observability for details.</p>"},{"location":"advanced-administration/observability/#high-availability-for-maas-metrics","title":"High Availability for MaaS Metrics","text":"<p>For production deployments where metric persistence across pod restarts and scaling events is critical, you should configure Limitador to use Redis as a backend storage solution.</p>"},{"location":"advanced-administration/observability/#why-high-availability-matters","title":"Why High Availability Matters","text":"<p>By default, Limitador stores rate-limiting counters in memory, which means:</p> <ul> <li>All hit counts are lost when pods restart</li> <li>Metrics reset when pods are rescheduled or scaled down</li> <li>No persistence across cluster maintenance or updates</li> </ul>"},{"location":"advanced-administration/observability/#setting-up-persistent-metrics","title":"Setting Up Persistent Metrics","text":"<p>To enable persistent metric counts, refer to the detailed guide:</p> <p>Configuring Redis storage for rate limiting</p> <p>This Red Hat documentation provides:</p> <ul> <li>Step-by-step Redis configuration for OpenShift</li> <li>Secret management for Redis credentials</li> <li>Limitador custom resource updates</li> <li>Production-ready setup instructions</li> </ul> <p>For local development and testing, you can also use our Limitador Persistence guide which includes a basic Redis setup script that works with any Kubernetes cluster.</p>"},{"location":"advanced-administration/observability/#visualization","title":"Visualization","text":"<p>For dashboard visualization options, see:</p> <ul> <li>OpenShift Monitoring: Monitoring overview</li> <li>Grafana on OpenShift: Red Hat OpenShift AI Monitoring</li> </ul>"},{"location":"advanced-administration/observability/#included-dashboards","title":"Included Dashboards","text":"<p>MaaS includes two Grafana dashboards for different personas:</p>"},{"location":"advanced-administration/observability/#platform-admin-dashboard","title":"Platform Admin Dashboard","text":"<p>Provides a comprehensive view of system health, usage across all users, and resource allocation:</p> Section Metrics Component Health Limitador up, Authorino pods, MaaS API pods, Gateway pods, Firing Alerts Key Metrics Total Tokens, Total Requests, Token Rate, Request Rate, Inference Success Rate, Active Users, P50 Response Latency, Rate Limit Ratio Auth Evaluation Auth Evaluation Latency (P50/P95/P99), Auth Success/Deny Rate Traffic Analysis Token/Request Rate by Model, Error Rates (4xx excl. 429, 5xx, 429 Rate Limited), Token/Request Rate by Tier, P95 Latency Error Breakdown Rate Limited Requests, Unauthorized Requests Model Metrics vLLM queue depth, inference latency, KV cache usage, token throughput, prompt vs generation token ratio, queue wait time, TTFT, ITL Top Users By token usage, by declined requests Detailed Breakdown Token Rate by User, Request Volume by User &amp; Tier Resource Allocation CPU/Memory/GPU per model pod <p>Template Variables</p> <p>The Platform Admin dashboard uses Grafana template variables for namespace filtering instead of hardcoded values. This allows the dashboard to adapt to different deployment configurations:</p> Variable Default Description <code>$datasource</code> <code>prometheus</code> Prometheus datasource <code>$maas_namespace</code> auto-detected MaaS API namespace (auto-detected from <code>kube_pod_info{pod=~\"maas-api.*\"}</code>) <code>$kuadrant_namespace</code> <code>kuadrant-system</code> Kuadrant components namespace <code>$gateway_namespace</code> <code>openshift-ingress</code> Istio/Gateway namespace <code>$llm_namespace</code> <code>llm</code> LLM model pods namespace <code>$model</code> <code>All</code> Filter by model name <p>To customize for your environment, change the variable values in Grafana's dashboard settings (gear icon \u2192 Variables).</p>"},{"location":"advanced-administration/observability/#ai-engineer-dashboard","title":"AI Engineer Dashboard","text":"<p>Personal usage view for individual developers:</p> Section Metrics Usage Summary My Total Tokens, My Total Requests, Token Rate, Request Rate, Rate Limit Ratio, Inference Success Rate Usage Trends Token Usage by Model, Usage Trends (tokens vs rate limited) Detailed Analysis Token Volume by Model, Rate Limited by Tier <p>Inference Success Rate</p> <p>Both dashboards use <code>rate()</code> on vLLM counters (<code>request_success_total</code>, <code>e2e_request_latency_seconds_count</code>) instead of raw counter values. This handles pod restarts correctly (counters reset independently and raw division produces incorrect results). When no traffic is present, <code>rate()/rate()</code> produces <code>NaN</code>; the dashboards use <code>((ratio) &gt;= 0) OR vector(1)</code> to filter <code>NaN</code> and default to 100% (healthy) when no traffic exists.</p> <p>Tokens vs Requests</p> <p>Both dashboards show token consumption (<code>authorized_hits</code>) for billing/cost tracking and request counts (<code>authorized_calls</code>) for capacity planning. Blue panels indicate request metrics; green panels indicate token metrics.</p> <p>Per-User Token Billing</p> <p>The Platform Admin dashboard shows token consumption aggregated by tier and model for system-level visibility. Per-user token consumption for billing is available via:</p> <ul> <li>AI Engineer dashboard: Individual users see their own token usage</li> <li>Prometheus API: Query <code>sum by (user) (increase(authorized_hits[24h]))</code> for billing periods</li> <li>RFE: A dedicated <code>/maas-api/v1/usage</code> chargeback API endpoint is recommended for production billing workflows</li> </ul>"},{"location":"advanced-administration/observability/#prerequisites","title":"Prerequisites","text":"<ul> <li>Grafana must be installed (for example via your observability team's process, a centralized instance, or the Grafana Operator). The dashboard helper does not install Grafana; it only deploys MaaS dashboard definitions and never fails (warnings only if none or multiple instances are found).</li> <li>Ensure the Grafana instance has label <code>app=grafana</code> so MaaS dashboard definitions attach.</li> <li>Configure a Prometheus or Thanos datasource in Grafana; the MaaS dashboards use the default Prometheus datasource.</li> </ul>"},{"location":"advanced-administration/observability/#deploying-dashboards","title":"Deploying Dashboards","text":"<p>Monitoring is installed by <code>install-observability.sh</code>. Dashboards are installed by a separate helper that discovers Grafana cluster-wide:</p> <pre><code>./scripts/install-grafana-dashboards.sh\n</code></pre> <p>Behavior: Scans for Grafana CRs cluster-wide. If one instance is found, deploys dashboards to that namespace and prints a success message. If none or multiple are found, prints a warning (and, for multiple, lists them) and exits without error. Use flags to target a specific instance:</p> <pre><code>./scripts/install-grafana-dashboards.sh --grafana-namespace maas-api\n./scripts/install-grafana-dashboards.sh --grafana-label app=grafana\n</code></pre> <p>To deploy only the dashboard manifests manually (same namespace as your Grafana):</p> <pre><code>kustomize build deployment/components/observability/dashboards | \\\n  sed \"s/namespace: maas-api/namespace: &lt;your-namespace&gt;/g\" | \\\n  kubectl apply -f -\n</code></pre>"},{"location":"advanced-administration/observability/#sample-dashboard-json","title":"Sample Dashboard JSON","text":"<p>For manual import, a sample dashboard JSON file is available:</p> <ul> <li>MaaS Token Metrics Dashboard</li> </ul> <p>To import into Grafana:</p> <ol> <li>Go to Grafana \u2192 Dashboards \u2192 Import</li> <li>Upload the JSON file or paste content</li> <li>Select your Prometheus datasource</li> </ol>"},{"location":"advanced-administration/observability/#key-metrics-reference","title":"Key Metrics Reference","text":""},{"location":"advanced-administration/observability/#token-and-request-metrics","title":"Token and Request Metrics","text":"Metric Description Labels <code>authorized_hits</code> Total tokens consumed (input + output combined, from <code>usage.total_tokens</code> in model responses) <code>user</code>, <code>tier</code>, <code>model</code> <code>authorized_calls</code> Total requests allowed <code>user</code>, <code>tier</code> <code>limited_calls</code> Total requests rate-limited <code>user</code>, <code>tier</code> <p>When to use which metric</p> <ul> <li>Billing/Cost: Use <code>authorized_hits</code> - represents actual token consumption, with <code>model</code> label for per-model breakdown</li> <li>API Usage: Use <code>authorized_calls</code> - represents number of API calls (per user, per tier)</li> <li>Rate Limiting: Use <code>limited_calls</code> - shows quota violations (per user, per tier)</li> </ul> <p>Total tokens only (input/output split not yet available)</p> <p>Token consumption is reported as total tokens (prompt + completion) per request. The pipeline reads <code>usage.total_tokens</code> from the model response via Kuadrant's TokenRateLimitPolicy. Separate input-token (<code>prompt_tokens</code>) and output-token (<code>completion_tokens</code>) counters are not yet available at the gateway level; this would require upstream changes in the Kuadrant wasm-shim to send separate <code>hits_addend</code> values for each token type. Chargeback and usage tracking per user, per subscription (tier), and per model are supported using <code>authorized_hits</code>.</p>"},{"location":"advanced-administration/observability/#latency-metrics","title":"Latency Metrics","text":"Metric Description Labels <code>istio_request_duration_milliseconds_bucket</code> Gateway-level latency histogram <code>destination_service_name</code>, <code>tier</code> <code>vllm:e2e_request_latency_seconds</code> Model inference latency <code>model_name</code>"},{"location":"advanced-administration/observability/#per-tier-latency-tracking","title":"Per-Tier Latency Tracking","text":"<p>The MaaS Platform uses an Istio Telemetry resource to add a <code>tier</code> dimension to gateway latency metrics. This enables tracking request latency per access tier (e.g. free, premium, enterprise). Gateway latency is labeled by tier only (not by user) to keep metric cardinality bounded and to align with latency-by-tier requirements (e.g. P50/P95/P99 per tier). Per-user metrics remain available from Limitador (<code>authorized_hits</code>, <code>authorized_calls</code>, <code>limited_calls</code>).</p> <p>How it works:</p> <ol> <li>The <code>gateway-auth-policy</code> injects the <code>X-MaaS-Tier</code> header from the resolved tier</li> <li>The Istio Telemetry resource extracts this header and adds it as a <code>tier</code> label to the <code>REQUEST_DURATION</code> metric</li> <li>Prometheus scrapes these metrics from the Istio gateway</li> </ol> <p>Configuration (<code>deployment/base/observability/istio-telemetry.yaml</code>):</p> <pre><code>apiVersion: telemetry.istio.io/v1\nkind: Telemetry\nmetadata:\n  name: latency-per-tier\n  namespace: openshift-ingress\nspec:\n  selector:\n    matchLabels:\n      gateway.networking.k8s.io/gateway-name: maas-default-gateway\n  metrics:\n  - providers:\n    - name: prometheus\n    overrides:\n    - match:\n        metric: REQUEST_DURATION\n        mode: CLIENT_AND_SERVER\n      tagOverrides:\n        tier:\n          operation: UPSERT\n          value: request.headers[\"x-maas-tier\"]\n</code></pre> <p>Security</p> <p>The <code>X-MaaS-Tier</code> header should be injected server-side by AuthPolicy. Ensure your AuthPolicy injects this header from the tier lookup (not client input) for accurate metrics attribution.</p>"},{"location":"advanced-administration/observability/#common-queries","title":"Common Queries","text":"<p>Token-based queries (billing/cost):</p> <pre><code># Total tokens consumed per user\nsum by (user) (authorized_hits)\n\n# Token consumption rate per model (tokens/sec)\nsum by (model) (rate(authorized_hits[5m]))\n\n# Top 10 users by tokens consumed\ntopk(10, sum by (user) (authorized_hits))\n\n# Token consumption by tier\nsum by (tier) (authorized_hits)\n</code></pre> <p>Request-based queries (capacity/usage):</p> <pre><code># Total requests per user\nsum by (user) (authorized_calls)\n\n# Request rate per tier (requests/sec)\nsum by (tier) (rate(authorized_calls[5m]))\n\n# Top 10 users by request count\ntopk(10, sum by (user) (authorized_calls))\n</code></pre> <p>Inference success rate (system health \u2014 did requests that reached the model succeed?):</p> <pre><code># Inference success rate using rate() to handle counter resets correctly\n# The &gt;= 0 filter removes NaN (0/0 when no traffic), falling back to vector(1) = 100%\n((sum(rate(vllm:request_success_total[5m])) / sum(rate(vllm:e2e_request_latency_seconds_count[5m]))) &gt;= 0) OR vector(1)\n</code></pre> <p>Rate limiting metrics (capacity planning \u2014 are users exceeding their quotas?):</p> <pre><code># Rate limit ratio (percentage of requests rejected by rate limiting)\n(sum(limited_calls) / (sum(authorized_calls) + sum(limited_calls))) OR vector(0)\n\n# Rate limit ratio by tier\n(sum by (tier) (limited_calls) / (sum by (tier) (authorized_calls) + sum by (tier) (limited_calls))) OR vector(0)\n\n# Rate limit violations per second by tier\nsum by (tier) (rate(limited_calls[5m]))\n\n# Users hitting rate limits most\ntopk(10, sum by (user) (limited_calls))\n</code></pre> <p>Latency queries:</p> <pre><code># P99 latency by service\nhistogram_quantile(0.99, sum by (destination_service_name, le) (rate(istio_request_duration_milliseconds_bucket[5m])))\n\n# P50 (median) latency\nhistogram_quantile(0.5, sum by (le) (rate(istio_request_duration_milliseconds_bucket[5m])))\n\n# P99 latency per tier\nhistogram_quantile(0.99, sum by (tier, le) (rate(istio_request_duration_milliseconds_bucket{tier!=\"\"}[5m])))\n</code></pre> <p>Filtering by tier</p> <p>For per-tier latency queries, use <code>tier!=\"\"</code> to exclude requests where the <code>X-MaaS-Tier</code> header was not injected. Token consumption metrics (<code>authorized_hits</code>, <code>authorized_calls</code>) from Limitador already only include successful requests.</p>"},{"location":"advanced-administration/observability/#maintenance","title":"Maintenance","text":""},{"location":"advanced-administration/observability/#grafana-datasource-token-rotation","title":"Grafana Datasource Token Rotation","text":"<p>The Grafana datasource uses a ServiceAccount token to authenticate with Prometheus. This token is valid for 30 days and must be rotated periodically.</p> <p>To rotate the token:</p> <pre><code># Delete the existing datasource and create a new one (or rotate the token per your Grafana setup).\n# To re-deploy only MaaS dashboard definitions: ./scripts/install-grafana-dashboards.sh\n</code></pre> <p>Production Recommendation</p> <p>For production deployments, consider automating token rotation using a CronJob or external secrets operator to avoid dashboard outages.</p>"},{"location":"advanced-administration/observability/#known-limitations","title":"Known Limitations","text":""},{"location":"advanced-administration/observability/#currently-blocked-features","title":"Currently Blocked Features","text":"<p>Some features require upstream changes and are currently blocked:</p> Feature Blocker Workaround <code>model</code> label on <code>authorized_calls</code> / <code>limited_calls</code> Kuadrant wasm-shim does not pass <code>responseBodyJSON</code> context for these counters Use <code>authorized_hits</code> for per-model breakdown; <code>authorized_calls</code>/<code>limited_calls</code> support per-user and per-tier Input/output token split Kuadrant TokenRateLimitPolicy sends a single <code>hits_addend</code> (total tokens); no mechanism for separate prompt/completion counters Total tokens available via <code>authorized_hits</code>; the response body contains <code>usage.prompt_tokens</code> and <code>usage.completion_tokens</code> but the wasm-shim does not split them Input/output token breakdown per user vLLM does not label its own metrics with <code>user</code> Total tokens per user available via <code>authorized_hits{user=\"...\"}</code>; vLLM prompt/generation token metrics are per-model only Kuadrant policy health metrics <code>kuadrant_policies_enforced</code>, <code>kuadrant_policies_total</code> etc. are defined in Kuadrant dev but not yet shipped in RHCL 1.x Enable <code>observability.enable: true</code> on the Kuadrant CR; the ServiceMonitors are created but policy-specific gauges will appear in a future operator release Authorino auth server metrics (upstream) The Kuadrant-provided <code>authorino-operator-monitor</code> only scrapes <code>/metrics</code> (controller-runtime); <code>/server-metrics</code> is not scraped by the upstream operator Resolved by MaaS: The <code>authorino-server-metrics</code> ServiceMonitor (deployed by <code>install-observability.sh</code>) scrapes <code>/server-metrics</code>. Auth evaluation latency and success/deny rate are visualized in the Platform Admin dashboard. maas-api application metrics The maas-api Go service does not expose a <code>/metrics</code> endpoint No workaround available. Metrics such as API key creation rate, token issuance rate, model discovery latency, and handler durations require adding Prometheus instrumentation to the Go service (e.g. <code>promhttp</code> handler, custom counters/histograms). PromQL \"name does not end in _total\" warnings Limitador metrics (<code>authorized_hits</code>, <code>authorized_calls</code>, <code>limited_calls</code>) and Authorino's <code>auth_server_authconfig_response_status</code> are counters but do not follow the Prometheus naming convention of ending in <code>_total</code>. When <code>rate()</code> is applied, Prometheus generates a warning that Grafana displays on panels. This is Grafana issue #84636 (open). The warnings are cosmetic and do not affect data correctness. All dashboard queries correctly apply <code>rate()</code> or <code>increase()</code> to these counters. The metric names are defined by upstream Kuadrant (Limitador) and Authorino \u2014 renaming requires upstream changes. <p>Total Tokens vs Token Breakdown</p> <p>Total token consumption per user is available via <code>authorized_hits{user=\"...\"}</code>. The blocked feature is the input/output split (prompt vs generation tokens) at the gateway level, which requires the wasm-shim to send two separate counter updates to Limitador.</p>"},{"location":"advanced-administration/observability/#available-per-user-and-per-tier-metrics","title":"Available Per-User and Per-Tier Metrics","text":"Feature Metric Label Latency per tier <code>istio_request_duration_milliseconds_bucket</code> <code>tier</code> Token consumption per user <code>authorized_hits</code> <code>user</code> Token consumption per tier <code>authorized_hits</code> <code>tier</code> Token consumption per model <code>authorized_hits</code> <code>model</code> Requests per user <code>authorized_calls</code> <code>user</code> Requests per tier <code>authorized_calls</code> <code>tier</code> Rate limited per user <code>limited_calls</code> <code>user</code> Rate limited per tier <code>limited_calls</code> <code>tier</code>"},{"location":"advanced-administration/observability/#requirements-alignment","title":"Requirements Alignment","text":"Requirement Status Notes Usage dashboards (token consumption per user, per subscription/tier, per model) Met Grafana dashboard + <code>authorized_hits</code> with <code>user</code>, <code>tier</code>, <code>model</code>; Prometheus scrapes Limitador <code>/metrics</code>. Latency by tier (P50/P95/P99) Met <code>istio_request_duration_milliseconds_bucket</code> with <code>tier</code> label; tier-only avoids unbounded cardinality. Request tracking (per user, per tier) Met <code>authorized_calls</code> with <code>user</code> and <code>tier</code> labels; <code>limited_calls</code> for rate-limit violations. Export for chargeback (CSV/API) Not provided (RFE) Per-user token data exists in Prometheus (<code>authorized_hits{user=\"...\"}</code>) but no dedicated billing API or export endpoint is implemented. RFE recommendation: Add <code>/maas-api/v1/usage</code> endpoint that queries Prometheus and returns per-user, per-tier, per-model token consumption in CSV/JSON for finance and chargeback systems. Input/output token split Not available Only total tokens (<code>authorized_hits</code>); separate input and output counters require upstream Kuadrant wasm-shim changes to send split <code>hits_addend</code> values. <code>model</code> label on request/rate-limit counters Partial <code>model</code> available on <code>authorized_hits</code> only; requires upstream Kuadrant fix to propagate <code>responseBodyJSON</code> context to <code>authorized_calls</code>/<code>limited_calls</code> counters. Policy enforcement health Future Kuadrant operator metrics (<code>kuadrant_policies_enforced</code>, <code>kuadrant_ready</code>, etc.) defined upstream but not yet shipped in RHCL 1.x; <code>limitador_up</code> and <code>datastore_partitioned</code> are available now. Auth evaluation metrics Met Authorino <code>/server-metrics</code> is scraped by the <code>authorino-server-metrics</code> ServiceMonitor. Auth evaluation latency (P50/P95/P99) and success/deny rate are available in the Platform Admin dashboard. maas-api application metrics Not available (gap) The maas-api Go service does not expose <code>/metrics</code>. API key creation rate, token issuance rate, and handler latency are not observable. Requires adding Prometheus instrumentation to the Go service."},{"location":"advanced-administration/storage-configuration/","title":"Storage Configuration","text":"<p>This guide explains the storage modes available for maas-api and how to configure them for different deployment scenarios.</p> <p>Note</p> <p>For External Database Setup Examples: If you need step-by-step instructions for setting up an external PostgreSQL database, see the external database samples.</p>"},{"location":"advanced-administration/storage-configuration/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Storage Modes</li> <li>Configuration Reference</li> <li>Choosing the Right Storage Mode</li> <li>Related Documentation</li> </ol>"},{"location":"advanced-administration/storage-configuration/#overview","title":"Overview","text":"<p>maas-api stores API key metadata and other persistent data. The storage backend can be configured based on your deployment requirements:</p> <ul> <li>Development/Testing: Use in-memory storage for simplicity</li> <li>Single-replica demos: Use disk storage for persistence without external dependencies</li> <li>Production with High Availability (HA): Use external PostgreSQL database</li> </ul>"},{"location":"advanced-administration/storage-configuration/#storage-modes","title":"Storage Modes","text":"<p>maas-api supports three storage modes, controlled by the <code>--storage</code> flag or <code>STORAGE_MODE</code> environment variable:</p> Mode Flag Value Description Persistence HA Support In-memory <code>in-memory</code> Ephemeral storage in application memory \u274c Data lost on restart \u274c Single replica only Disk <code>disk</code> SQLite database stored on local filesystem \u2705 Survives restarts \u274c Single replica only External <code>external</code> External PostgreSQL database \u2705 Full persistence \u2705 Multiple replicas"},{"location":"advanced-administration/storage-configuration/#in-memory-mode-default","title":"In-Memory Mode (Default)","text":"<p>Data is stored only in application memory. This is the default mode and requires no configuration.</p> <p>Use cases:</p> <ul> <li>Local development</li> <li>Quick testing</li> <li>Environments where persistence is not required</li> </ul> <p>Limitations:</p> <ul> <li>All data is lost when the pod restarts</li> <li>Cannot scale to multiple replicas</li> </ul>"},{"location":"advanced-administration/storage-configuration/#disk-mode","title":"Disk Mode","text":"<p>Data is persisted to a SQLite database file on the local filesystem.</p> <p>Use cases:</p> <ul> <li>Single-replica deployments</li> <li>Demos and proof-of-concept deployments</li> <li>Environments where an external database is not available</li> </ul> <p>Limitations:</p> <ul> <li>Cannot scale to multiple replicas (each replica would have its own database)</li> <li>Requires a PersistentVolumeClaim (PVC) for data to survive pod rescheduling</li> </ul>"},{"location":"advanced-administration/storage-configuration/#external-mode","title":"External Mode","text":"<p>Data is stored in an external PostgreSQL database, enabling full persistence and high availability.</p> <p>Use cases:</p> <ul> <li>Production deployments</li> <li>High availability requirements</li> <li>Multi-replica deployments</li> </ul> <p>Requirements:</p> <ul> <li>PostgreSQL database (version 12 or later recommended)</li> <li>Network connectivity from maas-api pods to the database</li> </ul>"},{"location":"advanced-administration/storage-configuration/#configuration-reference","title":"Configuration Reference","text":"<p>Configuration can be set via command-line flags or environment variables. They are interchangeable with the following precedence (highest to lowest):</p> <ol> <li>Command-line flags - override environment variables</li> <li>Environment variables - used if flag not provided</li> <li>Default values - used if neither is set</li> </ol> <p>Tip</p> <p>For Kubernetes deployments, environment variables are typically easier to configure via ConfigMaps or Secrets. Command-line flags are convenient for local development.</p> Flag Environment Variable Description Default <code>--storage</code> <code>STORAGE_MODE</code> Storage mode: <code>in-memory</code>, <code>disk</code>, or <code>external</code> <code>in-memory</code> <code>--db-connection-url</code> <code>DB_CONNECTION_URL</code> Database connection URL (required for <code>external</code> mode) - <code>--data-path</code> <code>DATA_PATH</code> Path to database file (for <code>disk</code> mode) <code>/data/maas-api.db</code>"},{"location":"advanced-administration/storage-configuration/#connection-pool-settings-external-mode-only","title":"Connection Pool Settings (External Mode Only)","text":"<p>These environment variables tune the database connection pool for external mode:</p> Variable Description Default <code>DB_MAX_OPEN_CONNS</code> Maximum number of open connections to the database <code>25</code> <code>DB_MAX_IDLE_CONNS</code> Maximum number of idle connections in the pool <code>5</code> <code>DB_CONN_MAX_LIFETIME_SECONDS</code> Maximum time (seconds) a connection can be reused <code>300</code>"},{"location":"advanced-administration/storage-configuration/#database-connection-url-format","title":"Database Connection URL Format","text":"<p>For external mode, the connection URL follows the standard PostgreSQL format:</p> <pre><code>postgresql://USER:PASSWORD@HOST:PORT/DATABASE?sslmode=MODE\n</code></pre> <p>Example:</p> <pre><code>postgresql://app:mypassword@postgres-service:5432/maas?sslmode=require\n</code></pre>"},{"location":"advanced-administration/storage-configuration/#choosing-the-right-storage-mode","title":"Choosing the Right Storage Mode","text":"Scenario Recommended Mode Notes Local development <code>in-memory</code> No setup required CI/CD pipelines <code>in-memory</code> Fast, no cleanup needed Single-replica demo <code>disk</code> Add a PVC for persistence Production <code>external</code> High availability"},{"location":"advanced-administration/storage-configuration/#related-documentation","title":"Related Documentation","text":"<ul> <li>External Database Setup Examples - Step-by-step guides for setting up PostgreSQL</li> </ul>"},{"location":"configuration-and-management/group-membership-known-issues/","title":"Group Membership Known Issues","text":"<p>This document describes known issues and side effects related to removing group membership from users during active usage in the MaaS Platform Technical Preview release.</p>"},{"location":"configuration-and-management/group-membership-known-issues/#group-membership-changes-during-active-usage","title":"Group Membership Changes During Active Usage","text":""},{"location":"configuration-and-management/group-membership-known-issues/#issue-description","title":"Issue Description","text":"<p>When a user is removed from a group (e.g., removed from <code>premium-users</code> group) while they have active tokens or ongoing requests, several side effects may occur due to the separation between user identity and Service Account identity.</p>"},{"location":"configuration-and-management/group-membership-known-issues/#how-group-membership-affects-access","title":"How Group Membership Affects Access","text":"<ol> <li>Token Request: When a user requests a MaaS token, their group memberships are evaluated to determine their tier.</li> <li>Service Account Creation: A Service Account is created in the tier-specific namespace (e.g., <code>maas-default-gateway-tier-premium</code>).</li> <li>Token Issuance: The token is issued for the Service Account, not the original user.</li> <li>Request Authorization: Requests are authorized based on the Service Account's identity and the tier metadata cached in the AuthPolicy.</li> </ol>"},{"location":"configuration-and-management/group-membership-known-issues/#side-effects","title":"Side Effects","text":""},{"location":"configuration-and-management/group-membership-known-issues/#1-existing-tokens-remain-valid","title":"1. Existing Tokens Remain Valid","text":"<p>Impact: High</p> <p>Description:</p> <p>When a user is removed from a group, their existing MaaS tokens remain valid until expiration because:</p> <ul> <li>The token is a Kubernetes Service Account token, not a user token.</li> <li>The Service Account continues to exist in the tier namespace.</li> <li>Kubernetes TokenReview validates the Service Account, not the original user's group membership.</li> </ul> <p>Example Scenario:</p> <pre><code>T+0h:   User \"alice\" is in \"premium-users\" group\nT+0h:   Alice requests a token -&gt; Gets SA token in maas-default-gateway-tier-premium namespace\nT+1h:   Admin removes Alice from \"premium-users\" group\nT+1h:   Alice's token is STILL VALID (expires at T+24h)\nT+1h:   Alice can still make requests using the existing token\nT+24h:  Token expires, Alice must request a new one\nT+24h:  New token request -&gt; Alice gets \"free\" tier (or fails if no tier matches)\n</code></pre> <p>Workaround:</p> <ul> <li>Revoke the user's tokens explicitly using the <code>DELETE /v1/tokens</code> endpoint.</li> <li>This deletes and recreates the user's Service Account, invalidating all existing tokens.</li> </ul> <pre><code>curl -X DELETE \"${HOST}/maas-api/v1/tokens\" \\\n  -H \"Authorization: Bearer ${USER_TOKEN}\"\n</code></pre> <p>Note: The user must authenticate with their own token to revoke their tokens. Administrators cannot revoke tokens on behalf of other users in the current implementation.</p>"},{"location":"configuration-and-management/group-membership-known-issues/#2-rate-limiting-continues-at-old-tier","title":"2. Rate Limiting Continues at Old Tier","text":"<p>Impact: Medium</p> <p>Description:</p> <p>The AuthPolicy caches the tier lookup result (default TTL: 5 minutes). After a user is removed from a group:</p> <ul> <li>Requests within the cache window continue to use the old tier's rate limits.</li> <li>After cache expiry, the tier is re-evaluated based on current group membership.</li> <li>If the user still has a valid token but no longer belongs to any tier group, requests may fail.</li> </ul> <p>Example Timeline:</p> <pre><code>T+0m:   User removed from \"premium-users\" group\nT+1m:   Request made -&gt; Cached tier \"premium\" used -&gt; Rate limit: 1000 tokens/min\nT+5m:   Cache expires\nT+6m:   Request made -&gt; Tier lookup fails (no matching group) -&gt; Request may fail with 403\n</code></pre> <p>Workaround:</p> <ul> <li>Wait for cache TTL (5 minutes) for rate limiting to reflect the new group membership.</li> <li>For immediate effect, restart Authorino pods (disruptive).</li> </ul>"},{"location":"configuration-and-management/group-membership-known-issues/#3-service-account-persists-after-group-removal","title":"3. Service Account Persists After Group Removal","text":"<p>Impact: Low</p> <p>Description:</p> <p>When a user is removed from a group, their Service Account in the tier namespace is not automatically deleted:</p> <ul> <li>The Service Account remains in the tier namespace.</li> <li>No new tokens can be issued for the old tier (tier lookup fails).</li> <li>Old tokens continue to work until expiration.</li> <li>This is a cleanup artifact, not a security issue (access is controlled by RBAC and rate limiting).</li> </ul> <p>Workaround:</p> <ul> <li>Service Accounts can be manually cleaned up if needed.</li> <li>The Service Account name is derived from the username: special characters are replaced with dashes, converted to lowercase, and an 8-character hash suffix is appended (e.g., <code>alice-example-com-a1b2c3d4</code> for <code>alice@example.com</code>).</li> <li>To find the Service Account for a specific user, list and filter by the username pattern:</li> </ul> <pre><code># List all Service Accounts in the tier namespace\nkubectl get serviceaccount -n maas-default-gateway-tier-&lt;old-tier&gt;\n\n# Filter by username pattern (e.g., for user \"alice@example.com\")\nkubectl get serviceaccount -n maas-default-gateway-tier-&lt;old-tier&gt; | grep alice\n\n# Delete the identified Service Account\nkubectl delete serviceaccount &lt;sa-name&gt; -n maas-default-gateway-tier-&lt;old-tier&gt;\n</code></pre>"},{"location":"configuration-and-management/group-membership-known-issues/#4-user-downgrade-creates-new-service-account","title":"4. User Downgrade Creates New Service Account","text":"<p>Impact: Low</p> <p>Description:</p> <p>When a user is moved to a lower tier (e.g., removed from <code>premium-users</code>, now only matching the <code>free</code> tier group, such as <code>system:authenticated</code> in the default configuration):</p> <ul> <li>A new Service Account is created in the new tier namespace (e.g., <code>maas-default-gateway-tier-free</code>).</li> <li>The old Service Account in the premium tier namespace remains.</li> <li>Old premium tokens continue to work until expiration.</li> <li>New token requests create tokens in the free tier namespace.</li> </ul> <p>Example:</p> <pre><code>Before: Alice in \"premium-users\" -&gt; SA in maas-default-gateway-tier-premium\nAfter:  Alice removed from \"premium-users\" (still matches \"free\" tier group)\n        -&gt; Old SA still exists in premium namespace\n        -&gt; New token request creates SA in maas-default-gateway-tier-free\n        -&gt; Alice now has SAs in both namespaces\n</code></pre> <p>Workaround:</p> <ul> <li>Revoke tokens before changing group membership to ensure clean transition.</li> <li>Delete the user's Service Account manually from the old tier namespace when they change groups.</li> </ul>"},{"location":"configuration-and-management/group-membership-known-issues/#5-monitoring-shows-split-metrics","title":"5. Monitoring Shows Split Metrics","text":"<p>Impact: Low</p> <p>Description:</p> <p>If a user has tokens from multiple tiers (before and after group change):</p> <ul> <li>Metrics are attributed to the Service Account's namespace.</li> <li>Usage appears split across tier namespaces.</li> <li>This is a reporting artifact and does not affect access control.</li> </ul> <p>Workaround:</p> <ul> <li>Aggregate metrics by username label if available.</li> <li>Encourage users to revoke old tokens after tier changes.</li> </ul>"},{"location":"configuration-and-management/group-membership-known-issues/#recommended-practices","title":"Recommended Practices","text":"<ol> <li> <p>Revoke Before Removing: When removing a user from a group, revoke their tokens first to ensure immediate access termination.</p> </li> <li> <p>Communicate Changes: Notify users before group membership changes so they can plan for re-authentication.</p> </li> <li> <p>Use Short Token Expiration: Shorter token lifetimes reduce the window of continued access after group removal.</p> </li> <li> <p>Clean Up Service Accounts: When a user changes groups, manually delete their Service Account from the old tier namespace to prevent orphaned resources.</p> </li> </ol>"},{"location":"configuration-and-management/group-membership-known-issues/#related-documentation","title":"Related Documentation","text":"<ul> <li>Tier Configuration - How to configure tier-to-group mappings</li> <li>Token Management - Understanding token lifecycle and revocation</li> </ul>"},{"location":"configuration-and-management/model-access-behavior/","title":"Model Tier Access Behavior","text":"<p>This document describes the expected behaviors and operational considerations when modifying model tier access in the MaaS Platform Technical Preview release.</p>"},{"location":"configuration-and-management/model-access-behavior/#model-tier-access-changes-during-active-usage","title":"Model Tier Access Changes During Active Usage","text":""},{"location":"configuration-and-management/model-access-behavior/#overview","title":"Overview","text":"<p>When a model is removed from a tier's access list (by updating the <code>alpha.maas.opendatahub.io/tiers</code> annotation on an <code>LLMInferenceService</code> resource), access revocation takes effect immediately. This section describes the expected behaviors and considerations for administrators.</p>"},{"location":"configuration-and-management/model-access-behavior/#how-model-access-removal-works","title":"How Model Access Removal Works","text":"<ol> <li>Annotation Update: The administrator updates the <code>alpha.maas.opendatahub.io/tiers</code> annotation to remove a tier from the allowed list</li> <li>ODH Controller Processing: The ODH Controller detects the annotation change and updates RBAC resources</li> <li>RBAC Update: The RoleBinding for the removed tier is deleted, revoking POST permissions for that tier's service accounts</li> <li>Access Revocation: Users from the removed tier lose access to the model</li> </ol>"},{"location":"configuration-and-management/model-access-behavior/#expected-behaviors","title":"Expected Behaviors","text":""},{"location":"configuration-and-management/model-access-behavior/#1-impact-on-active-requests","title":"1. Impact on Active Requests","text":"<p>Access revocation prevents new requests immediately.</p> <p>Description:</p> <ul> <li>New Requests: Any request arriving after the RBAC update will be denied immediately.</li> <li>In-Flight Requests: Requests that have already passed the authorization gate typically complete successfully. However, dependent requests or long-running sessions requiring re-authorization will fail.</li> </ul> <p>Example Scenario:</p> <pre><code>1. User starts a long-running inference request (e.g., 2-minute generation)\n2. Administrator removes the tier from model annotation at 30 seconds\n3. ODH Controller updates RBAC at 45 seconds\n4. Request may fail at next authorization checkpoint (if any)\n</code></pre> <p>Workaround:</p> <ul> <li>Avoid removing tier access during peak usage periods</li> <li>Monitor active requests before making changes</li> <li>Consider using maintenance windows for tier access changes</li> </ul>"},{"location":"configuration-and-management/model-access-behavior/#2-rbac-propagation-delay","title":"2. RBAC Propagation Delay","text":"<p>Description:</p> <ul> <li>There is a delay between annotation update and RBAC resource update by the ODH Controller</li> <li>During this window (typically seconds to minutes), access behavior is inconsistent:</li> <li>Some requests may still succeed (if authorization was cached)</li> <li>New requests may fail immediately</li> <li>Model may still appear in user's model list but be inaccessible</li> </ul> <p>Example Timeline:</p> <pre><code>T+0s:  Annotation updated (remove \"premium\" tier)\nT+5s:  ODH Controller detects change\nT+10s: RoleBinding deleted\nT+15s: RBAC fully propagated to API server\n</code></pre> <p>Workaround:</p> <ul> <li>Wait 1-2 minutes after annotation update before verifying access changes</li> <li>Monitor ODH Controller logs to confirm RBAC updates are complete</li> <li>Use <code>kubectl get rolebinding -n &lt;model-namespace&gt;</code> to verify RoleBinding removal</li> </ul>"},{"location":"configuration-and-management/model-access-behavior/#3-model-list-visibility-vs-access-mismatch","title":"3. Model List Visibility vs. Access Mismatch","text":"<p>Description:</p> <ul> <li>The <code>/v1/models</code> endpoint lists all models that are part of the MaaS instance (via gateway references)</li> <li>The endpoint does not filter models by tier access permissions</li> <li>Users may see models in the list that they can no longer access after tier removal</li> <li>Attempts to use these models will fail with <code>403 Forbidden</code> or <code>401 Unauthorized</code></li> </ul> <p>Example:</p> <pre><code>// GET /v1/models returns:\n{\n  \"data\": [\n    {\"id\": \"model-a\", \"ready\": true},  // Still accessible\n    {\"id\": \"model-b\", \"ready\": true}   // No longer accessible after tier removal\n  ]\n}\n\n// POST to model-b fails with 403\n</code></pre> <p>Workaround:</p> <p>This behavior will be resolved in a future release where the model list is filtered by tier permissions (see PR #294). In the meantime, clients should expect potential <code>403 Forbidden</code> errors if attempting to access models that appear in the list but are not permitted.</p>"},{"location":"configuration-and-management/model-access-behavior/#4-token-validity-vs-model-access-expected-behavior","title":"4. Token Validity vs. Model Access (Expected Behavior)","text":"<p>Tokens are per-user (Service Account), not per-model. Token validity and model access are independent\u2014this is by design.</p> <p>Description:</p> <ul> <li>Service Account tokens issued before tier removal remain valid until expiration</li> <li>Model access is controlled by RBAC, which is updated independently of token validity</li> <li>When a model is removed from a tier, the RBAC change revokes access immediately</li> <li>Users do not need to request new tokens; their existing tokens simply have access to fewer models</li> </ul> <p>Example:</p> <pre><code>1. User receives token at T+0 (valid for 1 hour)\n2. User has access to models A, B, C (via RBAC)\n3. Model B removed from tier at T+30min (RBAC updated)\n4. Token still valid, but model access changes:\n   - Model A: \u2705 Accessible (RBAC allows)\n   - Model B: \u274c No longer accessible (RBAC denies)\n   - Model C: \u2705 Accessible (RBAC allows)\n</code></pre> <p>User Communication:</p> <ul> <li>Clearly message users when a model is being removed from a tier to set expectations regarding token validity vs. model access.</li> </ul>"},{"location":"configuration-and-management/model-access-behavior/#5-immediate-access-revocation","title":"5. Immediate Access Revocation","text":"<p>Description:</p> <ul> <li>The platform does not provide a \"drain\" mechanism to allow existing users to finish their sessions while blocking new ones.</li> <li>Revocation applies to the authorization policy immediately.</li> <li>While in-flight requests often complete (as they have passed the gate), the user experience is an immediate loss of access for any subsequent interaction.</li> </ul> <p>Workaround:</p> <ul> <li>Monitor active requests before making changes:</li> </ul> <pre><code># Check for active connections (example)\nkubectl top pods -n &lt;model-namespace&gt;\n</code></pre> <ul> <li>Use maintenance windows for tier access changes</li> <li>Consider implementing request draining in future releases</li> </ul>"},{"location":"configuration-and-management/model-access-behavior/#recommended-practices","title":"Recommended Practices","text":"<ol> <li>Plan Tier Access Changes:</li> <li>Schedule changes during low-usage periods</li> <li>Notify affected users in advance when possible</li> <li> <p>Monitor active requests before making changes</p> </li> <li> <p>Verify Changes:</p> </li> <li> <p>Wait 1-2 minutes after annotation update</p> </li> <li> <p>Verify RoleBinding removal:</p> <pre><code>kubectl get rolebinding -n &lt;model-namespace&gt; | grep &lt;tier-name&gt;\n</code></pre> </li> <li> <p>Test access with a token from the affected tier</p> </li> <li> <p>Monitor for Issues:</p> </li> <li>Check ODH Controller logs for RBAC update errors</li> <li>Monitor API server logs for authorization failures</li> <li> <p>Watch for increased error rates in user applications</p> </li> <li> <p>Handle Errors Gracefully:</p> </li> <li>Implement retry logic with exponential backoff</li> <li>Provide clear error messages to end users</li> <li>Log access denials for troubleshooting</li> </ol>"},{"location":"configuration-and-management/model-access-behavior/#future-enhancements","title":"Future Enhancements","text":"<p>The following improvements are planned for future releases:</p> <ol> <li>Graceful Shutdown: Implement request draining before access revocation</li> <li>Model List Filtering: Filter <code>/v1/models</code> by tier permissions</li> <li>Real-time Notifications: Notify users when tier access changes</li> <li>Audit Logging: Enhanced logging for tier access changes</li> </ol>"},{"location":"configuration-and-management/model-access-behavior/#related-documentation","title":"Related Documentation","text":"<ul> <li>Tier Configuration - How to configure tier access</li> <li>Model Setup - How to configure model tier annotations</li> <li>Token Management - Understanding token lifecycle</li> </ul>"},{"location":"configuration-and-management/model-setup/","title":"Model Setup Guide","text":"<p>This guide explains how to configure <code>LLMInferenceService</code> resources to be picked up by the MaaS platform for authentication, rate limiting, and token-based consumption tracking.</p>"},{"location":"configuration-and-management/model-setup/#gateway-architecture","title":"Gateway Architecture","text":"<p>The MaaS platform uses a segregated gateway approach, where models explicitly opt-in to MaaS capabilities by referencing the <code>maas-default-gateway</code>. This provides flexibility and isolation between different model deployment scenarios.</p> <pre><code>%%{init: {'theme':'base', 'themeVariables': { 'fontSize':'16px', 'fontFamily':'system-ui, -apple-system, sans-serif', 'edgeLabelBackground':'transparent', 'labelBackground':'transparent', 'tertiaryColor':'transparent'}}}%%\ngraph TB\n    subgraph cluster[\"OpenShift/K8s Cluster\"]\n        subgraph gateways[\"Gateway Layer\"]\n            defaultGW[\"Default Gateway&lt;br/&gt;(ODH/KServe)&lt;br/&gt;&lt;br/&gt;\u2713 Existing auth model&lt;br/&gt;\u2713 No rate limits&lt;br/&gt;\"]\n            maasGW[\"MaaS Gateway&lt;br/&gt;(maas-default-gateway)&lt;br/&gt;&lt;br/&gt;\u2713 Token authentication&lt;br/&gt;\u2713 Tier-based rate limits&lt;br/&gt;\u2713 Token consumption \"]\n        end\n\n        subgraph models[\"Model Deployments\"]\n            standardModel[\"LLMInferenceService&lt;br/&gt;(Standard)&lt;br/&gt;&lt;br/&gt;spec:&lt;br/&gt;  model: ...&lt;br/&gt;  # Managed default Gateway instance\"]\n            maasModel[\"LLMInferenceService&lt;br/&gt;(MaaS-enabled)&lt;br/&gt;&lt;br/&gt;spec:&lt;br/&gt;  model: ...&lt;br/&gt;  router:&lt;br/&gt;    gateway:&lt;br/&gt;      refs:&lt;br/&gt;        - name: maas-default-gateway\"]\n        end\n\n        defaultGW -.-&gt;|Routes to| standardModel\n        maasGW ==&gt;|Routes to| maasModel\n    end\n\n    users[\"Users/Clients\"] --&gt;|Default ODH auth| defaultGW\n    apiUsers[\"API Clients\"] --&gt;|Bearer token| maasGW\n\n    style defaultGW fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff\n    style maasGW fill:#f57c00,stroke:#e65100,stroke-width:3px,color:#fff\n    style standardModel fill:#78909c,stroke:#546e7a,stroke-width:3px,color:#fff\n    style maasModel fill:#ffa726,stroke:#f57c00,stroke-width:3px,color:#fff\n    style cluster fill:none,stroke:#666,stroke-width:2px\n    style gateways fill:none,stroke:#5c6bc0,stroke-width:2px\n    style models fill:none,stroke:#5c6bc0,stroke-width:2px</code></pre> <p>Note</p> <p>The <code>maas-default-gateway</code> is created automatically during MaaS platform installation. You don't need to create it manually.</p>"},{"location":"configuration-and-management/model-setup/#benefits","title":"Benefits","text":"<ul> <li>Flexibility: Different models can have different security and access requirements</li> <li>Progressive Adoption: Teams can adopt MaaS features incrementally</li> <li>Production Control: Production models get full policy enforcement if needed</li> <li>Multi-Tenancy: Different teams can use different gateways in the same cluster</li> <li>Blast Radius Containment: Issues with one gateway don't affect the other</li> </ul>"},{"location":"configuration-and-management/model-setup/#prerequisites","title":"Prerequisites","text":"<p>Before configuring a model for MaaS, ensure you have:</p> <ul> <li>MaaS platform installed with <code>maas-default-gateway</code> deployed</li> <li>LLMInferenceService resource created or planned</li> <li>Cluster admin or equivalent permissions to modify <code>LLMInferenceService</code> resources</li> </ul>"},{"location":"configuration-and-management/model-setup/#configuring-models-for-maas","title":"Configuring Models for MaaS","text":"<p>To make your model available through the MaaS platform, you need to:</p> <ol> <li>Reference the maas-default-gateway in your <code>LLMInferenceService</code> spec</li> <li>Add the tier annotation to enable automatic RBAC setup</li> </ol>"},{"location":"configuration-and-management/model-setup/#step-1-add-gateway-reference","title":"Step 1: Add Gateway Reference","text":"<p>Configure your <code>LLMInferenceService</code> to use the <code>maas-default-gateway</code> by adding the gateway reference in the <code>router</code> section:</p> <pre><code>apiVersion: serving.kserve.io/v1alpha1\nkind: LLMInferenceService\nmetadata:\n  name: my-production-model\n  namespace: llm\nspec:\n  model:\n    uri: hf://Qwen/Qwen3-0.6B\n    name: Qwen/Qwen3-0.6B\n  replicas: 1\n\n  # Connect to MaaS-enabled gateway\n  router:\n    route: { }\n    gateway:\n      refs:\n        - name: maas-default-gateway\n          namespace: openshift-ingress\n\n  template:\n    # ... container configuration ...\n</code></pre> <p>Key Points:</p> <ul> <li>The <code>router.gateway.refs</code> field specifies which gateway to use</li> <li>Use <code>name: maas-default-gateway</code> and <code>namespace: openshift-ingress</code></li> <li>Without this specification, the model uses the default KServe gateway and is not subject to MaaS policies</li> </ul>"},{"location":"configuration-and-management/model-setup/#step-2-configure-tier-access-with-annotation","title":"Step 2: Configure Tier Access with Annotation","text":"<p>Add the <code>alpha.maas.opendatahub.io/tiers</code> annotation to enable automatic RBAC setup for tier-based access:</p> <pre><code>apiVersion: serving.kserve.io/v1alpha1\nkind: LLMInferenceService\nmetadata:\n  name: my-production-model\n  namespace: llm\n  annotations:\n    alpha.maas.opendatahub.io/tiers: '[]'\nspec:\n  # ... rest of spec ...\n</code></pre> <p>Annotation Values:</p> <ul> <li>Empty list <code>[]</code>: Grants access to all tiers (recommended for most models)</li> <li>List of tier names: Grants access to specific tiers only</li> <li>Example: <code>'[\"premium\",\"enterprise\"]'</code> - only premium and enterprise tiers can access</li> <li>Missing annotation: No tiers have access by default (model won't be accessible via MaaS)</li> </ul> <p>Examples:</p> <p>Allow all tiers:</p> <pre><code>annotations:\n  alpha.maas.opendatahub.io/tiers: '[]'\n</code></pre> <p>Allow specific tiers:</p> <pre><code>annotations:\n  alpha.maas.opendatahub.io/tiers: '[\"premium\",\"enterprise\"]'\n</code></pre>"},{"location":"configuration-and-management/model-setup/#what-the-annotation-does","title":"What the Annotation Does","text":"<p>This annotation automatically creates the necessary RBAC resources (Roles and RoleBindings) that allow tier-specific service accounts to POST to your <code>LLMInferenceService</code>. The ODH Controller handles this automatically when the annotation is present.</p> <p>Behind the scenes, it creates:</p> <ul> <li>Role: Grants <code>POST</code> permission on <code>llminferenceservices</code> resource</li> <li>RoleBinding: Binds tier service account groups (e.g., <code>system:serviceaccounts:maas-default-gateway-tier-premium</code>) to the role</li> </ul>"},{"location":"configuration-and-management/model-setup/#complete-example","title":"Complete Example","text":"<p>Here's a complete example of a MaaS-enabled model:</p> <pre><code>apiVersion: serving.kserve.io/v1alpha1\nkind: LLMInferenceService\nmetadata:\n  name: qwen3-model\n  namespace: llm\n  annotations:\n    alpha.maas.opendatahub.io/tiers: '[]'\nspec:\n  model:\n    uri: hf://Qwen/Qwen3-0.6B\n    name: Qwen/Qwen3-0.6B\n  replicas: 1\n  router:\n    route: { }\n    gateway:\n      refs:\n        - name: maas-default-gateway\n          namespace: openshift-ingress\n  template:\n    containers:\n      - name: main\n        image: \"vllm/vllm-openai:latest\"\n        resources:\n          limits:\n            nvidia.com/gpu: \"1\"\n            memory: 12Gi\n          requests:\n            nvidia.com/gpu: \"1\"\n            memory: 8Gi\n</code></pre>"},{"location":"configuration-and-management/model-setup/#updating-existing-models","title":"Updating Existing Models","text":"<p>To convert an existing model to use MaaS:</p>"},{"location":"configuration-and-management/model-setup/#method-1-patch-the-model","title":"Method 1: Patch the Model","text":"<pre><code>kubectl patch llminferenceservice my-production-model -n llm --type='json' -p='[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/router/gateway/refs/-\",\n    \"value\": {\n      \"name\": \"maas-default-gateway\",\n      \"namespace\": \"openshift-ingress\"\n    }\n  }\n]'\n\n# Add the tier annotation\nkubectl annotate llminferenceservice my-production-model -n llm \\\n  alpha.maas.opendatahub.io/tiers='[]' \\\n  --overwrite\n</code></pre>"},{"location":"configuration-and-management/model-setup/#method-2-edit-the-resource","title":"Method 2: Edit the Resource","text":"<pre><code>kubectl edit llminferenceservice my-production-model -n llm\n</code></pre> <p>Then add:</p> <ul> <li>Gateway reference in <code>spec.router.gateway.refs</code></li> <li>Annotation <code>alpha.maas.opendatahub.io/tiers</code> in <code>metadata.annotations</code></li> </ul>"},{"location":"configuration-and-management/model-setup/#verification","title":"Verification","text":"<p>After configuring your model, verify it's accessible through MaaS:</p> <p>1. Check the model appears in the models list:</p> <pre><code># Get your MaaS token first, then:\ncurl -sSk ${HOST}/maas-api/v1/models \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre> <p>2. Verify the model status:</p> <pre><code>kubectl get llminferenceservice my-production-model -n llm\n</code></pre> <p>3. Check RBAC was created (if using tier annotation):</p> <pre><code>kubectl get roles,rolebindings -n llm | grep my-production-model\n</code></pre> <p>4. Test inference request:</p> <pre><code># Use the MODEL_URL from the models list\ncurl -sSk -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"my-production-model\", \"prompt\": \"Hello\", \"max_tokens\": 50}' \\\n  \"${MODEL_URL}\"\n</code></pre>"},{"location":"configuration-and-management/model-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuration-and-management/model-setup/#model-not-appearing-in-maas-apiv1models","title":"Model Not Appearing in /maas-api/v1/models","text":"<ul> <li>Verify the gateway reference is correct: <code>name: maas-default-gateway</code>, <code>namespace: openshift-ingress</code></li> <li>Check that the model's status shows it's ready</li> <li>Ensure the model namespace is accessible (some configurations may restrict discovery)</li> </ul>"},{"location":"configuration-and-management/model-setup/#401-unauthorized-when-accessing-model","title":"401 Unauthorized When Accessing Model","text":"<ul> <li>Verify the tier annotation is set: <code>alpha.maas.opendatahub.io/tiers: '[]'</code> (or specific tiers)</li> <li>Check that your token's tier matches one of the tiers allowed in the annotation</li> <li>Verify RBAC resources were created: <code>kubectl get roles,rolebindings -n &lt;model-namespace&gt;</code></li> </ul>"},{"location":"configuration-and-management/model-setup/#403-forbidden-when-accessing-model","title":"403 Forbidden When Accessing Model","text":"<ul> <li>Ensure the tier annotation includes your tier</li> <li>Check that RBAC was properly created for your tier</li> <li>Verify the service account in your token has the correct tier namespace</li> </ul> <p>Removing Models from Tiers During Active Usage</p> <p>When updating the <code>alpha.maas.opendatahub.io/tiers</code> annotation to remove a tier, be aware that active requests may be affected. See Model Tier Access Behavior for details on expected behaviors and recommended practices.</p>"},{"location":"configuration-and-management/model-setup/#references","title":"References","text":"<ul> <li>Tier Management - Learn about configuring tier access</li> <li>Tier Configuration - Detailed tier setup instructions</li> <li>Model Tier Access Behavior - Expected behaviors and operational considerations</li> <li>Architecture Overview - Understand the overall MaaS architecture</li> <li>KServe LLMInferenceService Documentation - Official KServe documentation</li> </ul>"},{"location":"configuration-and-management/tier-concepts/","title":"Tier Concepts","text":"<p>This section provides reference information about how the tier system works.</p>"},{"location":"configuration-and-management/tier-concepts/#tier-membership-mapping","title":"Tier Membership Mapping","text":"<p>MaaS and Kubernetes administrators can defined the subscription levels using the <code>tier-to-group-mapping</code> ConfigMap in the <code>maas-api</code> namespace:</p> <p>tier-to-group-mapping.yaml ConfigMap example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tier-to-group-mapping\n  namespace: maas-api\ndata:\n  tiers: |\n    - name: free\n      description: Free tier for basic users\n      level: 1\n      groups:\n      - system:authenticated\n    - name: premium\n      description: Premium tier\n      level: 10\n      groups:\n      - premium-users\n    - name: enterprise\n      description: Enterprise tier\n      level: 20\n      groups:\n      - enterprise-users\n</code></pre>"},{"location":"configuration-and-management/tier-concepts/#configmap-field-breakdown","title":"ConfigMap Field Breakdown","text":"Field Purpose Default Value name The tier identifier used throughout the system. Must be unique and matches tier names in rate limit policies. <code>free</code>, <code>premium</code>, <code>enterprise</code> description Human-readable description of the tier's purpose and who it's intended for. Used for documentation and UI display. <code>Free tier for basic users</code>, <code>Enterprise tier for high-volume customers</code> level Numeric hierarchy for tier precedence. Higher numbers indicate higher tiers.  When a user belongs to multiple groups, the highest level tier is selected. <code>1</code> (lowest), <code>10</code> (medium), <code>20</code> (highest) groups Kubernetes groups whose members are assigned to this tier.  Users must be members of at least one group in the list to get this tier. <code>system:authenticated</code>, <code>premium-users</code>, <code>enterprise-users</code> <p>Important Notes:</p> <ul> <li>Users with multiple group memberships are assigned to the tier with the highest level number</li> <li>The <code>system:authenticated</code> group includes all authenticated users, commonly used for the free tier</li> <li>Group names must exist in your Kubernetes identity provider (LDAP, OIDC, etc.)</li> <li>Tier <code>name</code> values are case-sensitive and must match exactly with rate limit policy predicates</li> </ul>"},{"location":"configuration-and-management/tier-concepts/#tier-rate-limits-configuration","title":"Tier Rate Limits Configuration","text":"<p>MaaS and Kubernetes administrators can configure rate limits for each tier using the <code>RateLimitPolicy</code> custom resource.</p> <p>RateLimitPolicy.yaml example:</p> <pre><code>apiVersion: kuadrant.io/v1beta2\nkind: RateLimitPolicy\nmetadata:\n  name: model-rate-limits\n  namespace: llm\n</code></pre>"},{"location":"configuration-and-management/tier-concepts/#tier-namespaces","title":"Tier Namespaces","text":"<p>Each tier gets a dedicated namespace following the pattern <code>&lt;instance-name&gt;-tier-&lt;tier-name&gt;</code>:</p> <ul> <li><code>maas-default-gateway-tier-free</code></li> <li><code>maas-default-gateway-tier-premium</code></li> <li><code>maas-default-gateway-tier-enterprise</code></li> </ul>"},{"location":"configuration-and-management/tier-concepts/#tier-resolution-process","title":"Tier Resolution Process","text":"<ol> <li>User authenticates with JWT token</li> <li>Gateway extracts user groups from token</li> <li>MaaS API resolves tier based on group membership</li> <li>Tier information is cached for 5 minutes</li> <li>Access control and rate limiting are applied based on tier</li> </ol>"},{"location":"configuration-and-management/tier-configuration/","title":"Tier Configuration","text":"<p>This guide provides step-by-step instructions for configuring and managing tiers in the MaaS Platform.</p>"},{"location":"configuration-and-management/tier-configuration/#configuration-steps","title":"Configuration Steps","text":""},{"location":"configuration-and-management/tier-configuration/#1-configure-tier-mapping","title":"1. Configure Tier Mapping","text":"<p>Update <code>tier-to-group-mapping</code> ConfigMap:</p> <p>To add a new tier, save the current ConfigMap, modify it, and reapply:</p> <pre><code># 1. Edit ConfigMap (use example below as a guide)\nkubectl edit configmap tier-to-group-mapping -n maas-api\n\n# Example: Add this tier entry to the end of the tiers list:\n#   - name: stier\n#     description: S tier user\n#     level: 99\n#     groups:\n#     - fox\n</code></pre> <p>Verify the updated ConfigMap:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tier-to-group-mapping\n  namespace: maas-api\ndata:\n  tiers: |\n    - name: free\n      description: Free tier for basic users\n      level: 1\n      groups:\n      - system:authenticated\n    - name: premium\n      description: Premium tier\n      level: 10\n      groups:\n      - premium-users\n    - name: enterprise\n      description: Enterprise tier\n      level: 20\n      groups:\n      - enterprise-users\nEOF\n</code></pre> <p>Restart the MaaS API to pick up the new configuration:</p> <pre><code>kubectl rollout restart deployment/maas-api -n maas-api\n</code></pre> <p>Adding Users to Tiers</p> <p>To add a user to a tier, add them to the appropriate Kubernetes group. For example, to add a user to the <code>fox</code> group (which maps to the <code>stier</code> tier in this example):</p> <pre><code># Add a user to the fox group\nkubectl patch group fox -p '{\"users\": [\"username\"]}' --type merge\n</code></pre> <p>Replace <code>username</code> with the actual username. Users will automatically be assigned to the tier when they request a new token.</p>"},{"location":"configuration-and-management/tier-configuration/#2-configure-tier-access","title":"2. Configure Tier Access","text":"<p>Grant tier-specific access to models by annotating the <code>LLMInferenceService</code> resource with the <code>alpha.maas.opendatahub.io/tiers</code> annotation:</p> <pre><code>kubectl annotate llminferenceservice &lt;model-name&gt; -n llm \\\n  alpha.maas.opendatahub.io/tiers='[\"stier\",\"premium\",\"enterprise\"]' \\\n  --overwrite\n</code></pre> <p>Annotation Behavior:</p> <ul> <li>List of tier names: Grant access to specific tiers (e.g., <code>[\"stier\",\"premium\",\"enterprise\"]</code>)</li> <li>Empty list <code>[]</code>: Grant access to all tiers</li> <li>Missing annotation: No tiers have access by default</li> </ul> <p>Example - Grant access to stier and premium tiers:</p> <pre><code>kubectl annotate llminferenceservice qwen3 -n llm \\\n  alpha.maas.opendatahub.io/tiers='[\"stier\",\"premium\"]' \\\n  --overwrite\n</code></pre> <p>This annotation automatically sets up the necessary RBAC (Role and RoleBinding) for the specified tiers to access the model via MaaS tokens.</p> <p>Manual RBAC Setup</p> <p>For reference, here's what the automatic RBAC setup looks like behind the scenes if you need to configure access manually:</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: model-post-access\n  namespace: &lt;model-namespace&gt;\nrules:\n  - apiGroups: [\"serving.kserve.io\"]\n    resources: [\"llminferenceservices\"]\n    verbs: [\"post\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: model-post-access-tier-binding\n  namespace: &lt;model-namespace&gt;\nsubjects:\n  - kind: Group\n    name: system:serviceaccounts:maas-default-gateway-tier-&lt;tier&gt;\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: model-post-access\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Why the custom <code>post</code> verb?</p> <p>We intentionally use a custom verb (<code>post</code>) instead of standard Kubernetes verbs like <code>get</code> or <code>create</code>. This is the only RBAC permission required for model access. By using a non-standard verb that doesn't exist in Kubernetes' built-in authorization, we minimize the security surface - these service accounts cannot accidentally read, modify, or delete any cluster resources.</p>"},{"location":"configuration-and-management/tier-configuration/#3-configure-rate-limiting","title":"3. Configure Rate Limiting","text":"<p>Add tier-specific rate limits by patching the existing <code>gateway-token-rate-limits</code> TokenRateLimitPolicy:</p> <pre><code>kubectl patch tokenratelimitpolicy gateway-token-rate-limits -n openshift-ingress --type merge --patch-file=/dev/stdin &lt;&lt;'EOF'\nspec:\n  limits:\n    stier-user-tokens: # 1\n      rates:\n        - limit: 999 # 2\n          window: 1m # 3\n      when:\n        - predicate: auth.identity.tier == \"stier\" # 4\n      counters:\n        - expression: auth.identity.userid # 5\nEOF\n</code></pre> <p>Rate Limit Policy Configuration Explained:</p> <ol> <li>Tier definition - Each tier (free, premium, enterprise) gets its own configuration block (this is just a naming convention, it is not used for the actual tier resolution)</li> <li>Token limit - Maximum number of total tokens allowed per time window</li> <li>Time window - Duration after which the request counter resets</li> <li>Predicate condition - Determines when this tier's limits apply based on user authentication</li> <li>Counter expression - Tracks token consumption per user ID (globally)</li> </ol> <p>Important</p> <p>The predicate condition (not the Tier Definition) is used to determine when this tier's limits apply based on user authentication. It is a CEL expression that is evaluated by the Authorino policy engine.</p> <p>Validate the TokenRateLimitPolicy has been updated and enforced:</p> <pre><code># Delete the Kuadrant operator pod to trigger a re-sync\nkubectl delete pod -l control-plane=controller-manager -n kuadrant-system\n\n# Wait for the TokenRateLimitPolicy to be enforced\nkubectl wait --for=condition=Enforced=true tokenratelimitpolicy/gateway-token-rate-limits -n openshift-ingress --timeout=2m\n</code></pre>"},{"location":"configuration-and-management/tier-configuration/#4-validate-the-configuration","title":"4. Validate the Configuration","text":"<p>Configuration can be validated by logging in as a user belonging to the appropriate group and running through the manual validation steps in the deployment scripts documentation, or by using the automated validation script.</p> <pre><code># Validate the configuration with 20 requests and a max tokens limit of 500\n./scripts/validate-deployment.sh --rate-limit-requests 20 --max-tokens 500\n</code></pre> <p>Example Output:</p> <pre><code>\ud83d\udd0d Checking: Token information\n\u2139\ufe0f  Token subject: system:serviceaccount:maas-default-gateway-tier-stier:jland-78028f6d\n\u2705 PASS: User tier: stier &lt;--- Important\n\ud83d\udd0d Checking: Models endpoint\n\u2705 PASS: Models endpoint returns 200 OK\n...\n\ud83d\udd0d Checking: Rate limiting\n\u2139\ufe0f  Sending 20 rapid requests to test rate limiting...\n\u2705 PASS: Rate limiting is working (5 successful, 15 rate limited) &lt;--- Important\n</code></pre>"},{"location":"configuration-and-management/tier-configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuration-and-management/tier-configuration/#general-tips","title":"General Tips","text":"<p>Authentication errors (403/401): Check Authorino logs for detailed error messages:</p> <pre><code>kubectl logs -n openshift-ingress -l app.kubernetes.io/name=authorino --tail=50\n</code></pre> <p>Token retrieval issues: Check MaaS API logs during the token request:</p> <pre><code>kubectl logs -n maas-api -l app=maas-api --tail=50\n</code></pre> <p>Policy enforcement issues: Restart the Kuadrant operator to trigger policy re-sync:</p> <pre><code>kubectl delete pod -l control-plane=controller-manager -n kuadrant-system\n</code></pre>"},{"location":"configuration-and-management/tier-configuration/#common-issues","title":"Common Issues","text":""},{"location":"configuration-and-management/tier-configuration/#403-forbidden-not-authorized-unknown-reason","title":"403 Forbidden: \"not authorized: unknown reason\"","text":"<p>Possible Cause: Added new tier to ConfigMap but didn't update <code>gateway-token-rate-limits</code> TokenRateLimitPolicy.</p> <p>Fix: Validate/Update the TokenRateLimitPolicy as documented in Configure Rate Limiting, then restart the Kuadrant operator:</p> <pre><code>kubectl patch tokenratelimitpolicy gateway-token-rate-limits -n openshift-ingress --type merge --patch-file=/dev/stdin &lt;&lt;'EOF'\nspec:\n  limits:\n    &lt;tier-name&gt;-user-tokens:\n      rates:\n        - limit: 999\n          window: 1m\n      when:\n        - predicate: auth.identity.tier == \"&lt;tier-name&gt;\"\n      counters:\n        - expression: auth.identity.userid\nEOF\n\nkubectl delete pod -l control-plane=controller-manager -n kuadrant-system\n</code></pre> <p>Modifying Tiers During Active Usage</p> <p>Modifying the tier definitions (ConfigMap) while users have active requests may cause side effects due to caching and eventual consistency. See Tier Modification Known Issues for details on:</p> <ul> <li>Propagation delays for group changes</li> <li>Tier name immutability</li> <li>Monitoring inconsistencies</li> <li>Service interruptions on tier deletion</li> </ul> <p>Removing Group Membership During Active Usage</p> <p>Removing a user from a group while they have active tokens may not immediately revoke access. See Group Membership Known Issues for details on:</p> <ul> <li>Existing tokens remaining valid until expiration</li> <li>Rate limiting continuing at the old tier</li> <li>Service Account persistence after group removal</li> <li>Recommended practices for group membership changes</li> </ul> <p>Model Tier Access Changes</p> <p>Removing a model from a tier's access list (by updating the <code>alpha.maas.opendatahub.io/tiers</code> annotation) takes effect immediately. See Model Tier Access Behavior for details on:</p> <ul> <li>Expected behaviors when access is revoked</li> <li>RBAC propagation timing</li> <li>Recommended practices for tier access changes</li> </ul>"},{"location":"configuration-and-management/tier-modification-known-issues/","title":"Tier Modification Known Issues","text":"<p>This document describes known issues and side effects related to modifying tier definitions (ConfigMap) during active usage in the MaaS Platform Technical Preview release.</p>"},{"location":"configuration-and-management/tier-modification-known-issues/#tier-configuration-changes-during-active-usage","title":"Tier Configuration Changes During Active Usage","text":""},{"location":"configuration-and-management/tier-modification-known-issues/#issue-description","title":"Issue Description","text":"<p>When the <code>tier-to-group-mapping</code> ConfigMap is modified (e.g., changing groups or levels) while users are actively making requests, several side effects may occur due to caching and eventual consistency in the system.</p>"},{"location":"configuration-and-management/tier-modification-known-issues/#how-tier-resolution-works","title":"How Tier Resolution Works","text":"<ol> <li>ConfigMap: Tiers are defined in the <code>tier-to-group-mapping</code> ConfigMap.</li> <li>MaaS API: Watches the ConfigMap and updates its internal state. Used for token generation.</li> <li>AuthPolicy (Authorino): Caches tier lookup results for authenticated users (default TTL: 5 minutes).</li> <li>Token: Contains a Service Account identity associated with a specific tier namespace (e.g., <code>maas-default-gateway-tier-free</code>) at the time of issuance.</li> </ol>"},{"location":"configuration-and-management/tier-modification-known-issues/#side-effects","title":"Side Effects","text":""},{"location":"configuration-and-management/tier-modification-known-issues/#1-propagation-delay-for-group-changes","title":"1. Propagation Delay for Group Changes","text":"<p>Impact: Medium</p> <p>Description:</p> <p>If a user's group membership changes or a tier's group definition is updated:</p> <ul> <li>The <code>AuthPolicy</code> (Authorino) caches the user's tier for 5 minutes.</li> <li>The user will continue to be rate-limited according to their old tier until the cache expires.</li> <li>After the cache expires, the new tier limits will apply.</li> </ul> <p>Example Scenario:</p> <pre><code>T+0s:  User added to \"premium-users\" group (was \"free\")\nT+10s: ConfigMap updated in MaaS API\nT+1m:  User makes request -&gt; Authorino uses cached \"free\" tier (Rate Limit: 10/min)\nT+5m:  Cache expires\nT+6m:  User makes request -&gt; Authorino looks up tier -&gt; \"premium\" (Rate Limit: 1000/min)\n</code></pre> <p>Workaround:</p> <ul> <li>Wait for the cache TTL (5 minutes) for changes to fully propagate.</li> <li>Restart the Authorino pods to force immediate cache invalidation (disruptive).</li> </ul>"},{"location":"configuration-and-management/tier-modification-known-issues/#2-tier-names-are-immutable","title":"2. Tier Names Are Immutable","text":"<p>Important: Tier names (the <code>name</code> field in the ConfigMap) are expected to be immutable and should not be renamed after creation. This design ensures consistency across:</p> <ul> <li><code>RateLimitPolicy</code> and <code>TokenRateLimitPolicy</code> definitions</li> <li>Tier namespace naming (e.g., <code>maas-default-gateway-tier-free</code>)</li> <li>Token claims and Service Account associations</li> </ul> <p>If you need to change how a tier is displayed to users, use the <code>displayName</code> field instead. The <code>displayName</code> can be modified at any time without affecting the underlying tier configuration or policies.</p> <p>Example:</p> <pre><code># Correct: Change displayName, not name\ntiers:\n  - name: free           # Immutable - do not change\n    displayName: \"Starter Plan\"  # Can be changed for UI purposes\n    level: 1\n    groups:\n      - \"system:authenticated\"\n</code></pre>"},{"location":"configuration-and-management/tier-modification-known-issues/#3-monitoring-inconsistency","title":"3. Monitoring Inconsistency","text":"<p>Impact: Low</p> <p>Description:</p> <p>Tokens are issued with a Service Account in a tier-specific namespace (e.g., <code>maas-default-gateway-tier-free</code>). This namespace is embedded in the token claims. If a user moves to a new tier (e.g., <code>premium</code>) but continues using a valid token issued under the old tier:</p> <ul> <li>Enforcement: They get the new tier's rate limits (after cache expiry).</li> <li>Monitoring: Their usage metrics in Prometheus will still be attributed to the old Service Account/Namespace (<code>maas-default-gateway-tier-free</code>).</li> </ul> <p>Example:</p> <ul> <li>User upgrades to Premium.</li> <li>Token claim: <code>system:serviceaccount:maas-default-gateway-tier-free:user-123</code></li> <li>Rate Limit enforced: Premium (correct)</li> <li>Prometheus Metric: <code>requests_total{namespace=\"maas-default-gateway-tier-free\"}</code> (incorrect attribution)</li> </ul> <p>Workaround:</p> <ul> <li>Users must request a new token to have their usage correctly attributed to the new tier's namespace.</li> <li>This is a monitoring reporting issue only; access control is unaffected.</li> <li>Token Invalidation: Tokens can be invalidated by removing the old ServiceAccount associated with them. When a user moves to a new tier, their old ServiceAccount in the previous tier namespace remains (it is not automatically deleted). Administrators can manually delete these orphaned ServiceAccounts to invalidate any remaining tokens, but this is not required for normal operation.</li> </ul>"},{"location":"configuration-and-management/tier-modification-known-issues/#4-service-interruption-on-tier-deletion","title":"4. Service Interruption on Tier Deletion","text":"<p>Impact: Medium</p> <p>Description:</p> <p>If a tier is deleted from the ConfigMap while users are still assigned to it (and have no other matching tier):</p> <ul> <li>The <code>TierLookup</code> endpoint will return an error (e.g., 404 or GroupNotFound).</li> <li>The <code>AuthPolicy</code> relies on this metadata.</li> <li>Requests may fail with <code>403 Forbidden</code> or <code>500 Internal Server Error</code> depending on how the failure is handled in the policy.</li> </ul> <p>Workaround:</p> <ul> <li>Ensure users are moved to a new tier (via group changes) before deleting the old tier definition.</li> </ul>"},{"location":"configuration-and-management/tier-modification-known-issues/#recommended-practices","title":"Recommended Practices","text":"<ol> <li>Treat Tier Names as Immutable: Do not rename tiers after creation. Use <code>displayName</code> for UI-facing name changes.</li> <li>Update Policies First: When adding new tiers, update the <code>RateLimitPolicy</code> first.</li> <li>Plan for Delays: Expect a 5-minute delay for tier changes to affect active traffic.</li> <li>Token Refresh: Encourage users to refresh their tokens after significant tier changes to ensure correct monitoring attribution.</li> </ol>"},{"location":"configuration-and-management/tier-overview/","title":"Tier Management Overview","text":"<p>This guide explains how to configure and manage subscription tiers for the MaaS Platform. Tiers enable differentiated service levels with varying access permissions, rate limits, and quotas.</p>"},{"location":"configuration-and-management/tier-overview/#overview","title":"Overview","text":"<p>The tier system is driven by Kubernetes native objects and provides:</p> <ul> <li>Group-based access control: Users are assigned tiers based on their Kubernetes group membership</li> <li>Namespace-scoped RBAC: Each tier has its own namespace for permission management</li> <li>Dynamic tier resolution: User tiers are resolved on each request</li> <li>Per-model authorization: Access control is enforced at the model level</li> <li>Hierarchical precedence: Users with multiple group memberships get the highest tier</li> </ul>"},{"location":"configuration-and-management/tier-overview/#documentation-structure","title":"Documentation Structure","text":"<p>This tier management documentation is organized into three sections:</p> <ol> <li>Tier Overview (this document) - High-level overview of the tier system</li> <li>Tier Configuration - Step-by-step configuration guide</li> <li>Tier Concepts - Reference material explaining how the tier system works</li> </ol>"},{"location":"configuration-and-management/tier-overview/#quick-start","title":"Quick Start","text":"<p>To get started with tier management, see the Configuration Guide.</p> <p>For detailed information about how the tier system works internally, see the Tier Concepts documentation.</p>"},{"location":"configuration-and-management/tls-configuration/","title":"TLS Configuration","text":"<p>This guide covers TLS configuration for the MaaS API component to enable encrypted communication for all API traffic.</p> <p>Quick Verification</p> <p>For TLS verification commands, see Validation Guide - TLS Verification.</p>"},{"location":"configuration-and-management/tls-configuration/#overview","title":"Overview","text":"<p>The MaaS API supports end-to-end TLS encryption across all traffic paths:</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Gateway as Gateway&lt;br/&gt;(openshift-ingress)\n    participant Authorino as Authorino&lt;br/&gt;(kuadrant-system)\n    participant MaaS as maas-api\n\n    Note over Gateway,MaaS: All connections use TLS\n\n    Client-&gt;&gt;Gateway: HTTPS request\n    Gateway-&gt;&gt;Authorino: gRPC/TLS (port 50051)\n    Authorino-&gt;&gt;Gateway: Auth decision\n    Gateway-&gt;&gt;MaaS: HTTPS (port 8443)\n    MaaS-&gt;&gt;Gateway: Response\n    Gateway-&gt;&gt;Client: Response</code></pre> <p>When TLS is enabled:</p> <ul> <li>External API traffic is encrypted from client to backend</li> <li>Internal authentication traffic (Authorino \u2192 <code>maas-api</code>) is encrypted</li> <li>All certificate validation uses a trusted CA bundle</li> </ul> <p>Default deployment</p> <p>TLS is enabled by default when deploying via <code>./scripts/deploy.sh</code> (both operator and kustomize modes). The deployment relies on OpenShift service-ca-operator for automatic certificate provisioning. Use <code>--disable-tls-backend</code> to deploy with HTTP instead.</p>"},{"location":"configuration-and-management/tls-configuration/#prerequisites","title":"Prerequisites","text":""},{"location":"configuration-and-management/tls-configuration/#authorino-tls-configuration","title":"Authorino TLS Configuration","text":"<p>Authorino handles two TLS-protected traffic flows: </p> <ul> <li>inbound from the Gateway (listener TLS) </li> <li>outbound to <code>maas-api</code> (for metadata lookups). </li> </ul> <p>For ODH/RHOAI deployments, the inbound flow is a platform pre-requisite for secure <code>LLMInferenceService</code> communication; only the outbound configuration is needed for MaaS.</p> <p>For all deployments using <code>./scripts/deploy.sh</code> (both operator and kustomize modes with TLS enabled), both flows are configured automatically via <code>configure-authorino-tls.sh</code>.</p>"},{"location":"configuration-and-management/tls-configuration/#gateway-authorino-listener-tls","title":"Gateway \u2192 Authorino (Listener TLS)","text":"<p>Enables TLS on Authorino's gRPC listener for incoming authentication requests from the Gateway.</p> <pre><code># Annotate service for certificate generation\nkubectl annotate service authorino-authorino-authorization \\\n  -n kuadrant-system \\\n  service.beta.openshift.io/serving-cert-secret-name=authorino-server-cert \\\n  --overwrite\n\n# Patch Authorino CR to enable TLS listener\nkubectl patch authorino authorino -n kuadrant-system --type=merge --patch '\n{\n  \"spec\": {\n    \"listener\": {\n      \"tls\": {\n        \"enabled\": true,\n        \"certSecretRef\": {\n          \"name\": \"authorino-server-cert\"\n        }\n      }\n    }\n  }\n}'\n</code></pre> <p>For more details, see the ODH KServe TLS setup guide.</p>"},{"location":"configuration-and-management/tls-configuration/#configuring-the-gateway-for-authorino-tls","title":"Configuring the Gateway for Authorino TLS","text":"<p>When TLS is enabled on Authorino's listener, the Gateway must be configured to trust Authorino's certificate. Without service mesh sidecars (the default for OpenShift Ingress), this requires an EnvoyFilter to configure the upstream TLS context.</p> <p>For MaaS gateways managed by the ODH Model Controller, use the <code>security.opendatahub.io/authorino-tls-bootstrap</code> annotation to enable automatic EnvoyFilter creation:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: maas-default-gateway\n  namespace: openshift-ingress\n  annotations:\n    security.opendatahub.io/authorino-tls-bootstrap: \"true\"\n    opendatahub.io/managed: \"false\"  # Custom AuthPolicies managed externally\nspec:\n  # ... gateway spec ...\n</code></pre> Annotation Description <code>security.opendatahub.io/authorino-tls-bootstrap</code> When <code>\"true\"</code>, creates the EnvoyFilter for Gateway \u2192 Authorino TLS communication <code>opendatahub.io/managed</code> When <code>\"false\"</code>, disables automatic AuthPolicy creation (for custom policy management) <p>Interim solution</p> <p>This annotation is an interim solution until CONNLINK-528 ships native support for configuring TLS between the Gateway and Authorino without mesh sidecars. Previously, setting <code>opendatahub.io/managed=false</code> would skip both AuthPolicy and EnvoyFilter creation, leaving no way to configure Authorino TLS independently. The <code>authorino-tls-bootstrap</code> annotation decouples these concerns, allowing TLS configuration even when AuthPolicies are managed externally.</p>"},{"location":"configuration-and-management/tls-configuration/#authorino-maas-api-outbound-tls","title":"Authorino \u2192 maas-api (Outbound TLS)","text":"<p>Enables Authorino to make HTTPS calls to <code>maas-api</code> for tier metadata lookups. Requires the cluster CA bundle and SSL environment variables.</p> <pre><code># Configure SSL environment variables for outbound HTTPS\n# Note: The Authorino CR doesn't support envVars, so we patch the deployment directly\nkubectl -n kuadrant-system set env deployment/authorino \\\n  SSL_CERT_FILE=/etc/ssl/certs/openshift-service-ca/service-ca-bundle.crt \\\n  REQUESTS_CA_BUNDLE=/etc/ssl/certs/openshift-service-ca/service-ca-bundle.crt\n</code></pre> <p>Note</p> <p>OpenShift's service-ca-operator automatically populates the ConfigMap with the cluster CA certificate.</p>"},{"location":"configuration-and-management/tls-configuration/#gateway-maas-api-tls-destinationrule","title":"Gateway \u2192 maas-api TLS (DestinationRule)","text":"<p>The <code>tls</code> overlay includes a DestinationRule to configure TLS origination from the gateway to <code>maas-api</code>.</p> <p>Why DestinationRule? Gateway API's HTTPRoute doesn't tell Istio to use TLS when communicating with backends. Without BackendTLSPolicy (GA in Gateway API v1.4), an Istio-native DestinationRule is required to configure TLS origination.</p> <pre><code>Client \u2192 Gateway (TLS termination) \u2192 [DestinationRule] \u2192 maas-api:8443 (TLS origination)\n</code></pre> <p>Future consideration</p> <p>Once Gateway API v1.4+ with BackendTLSPolicy is supported by the Istio Gateway provider, the DestinationRule can be replaced with a standard Gateway API resource.</p>"},{"location":"configuration-and-management/tls-configuration/#custom-maas-api-tls-configuration","title":"Custom maas-api TLS Configuration","text":"<p>This section covers how <code>maas-api</code> is configured to use TLS certificates. These settings are automatically configured by the kustomize overlays; manual configuration is only needed for custom deployments or non-OpenShift environments.</p>"},{"location":"configuration-and-management/tls-configuration/#environment-variables","title":"Environment Variables","text":"<p>The <code>maas-api</code> component accepts TLS configuration via environment variables:</p> Variable Description Example <code>TLS_CERT</code> Path to TLS certificate file <code>/etc/maas-api/tls/tls.crt</code> <code>TLS_KEY</code> Path to TLS private key file <code>/etc/maas-api/tls/tls.key</code> <p>When both variables are set, the API server listens on HTTPS (port 8443) instead of HTTP (port 8080).</p>"},{"location":"configuration-and-management/tls-configuration/#volume-mounts","title":"Volume Mounts","text":"<p>Certificates should be mounted as a volume from a Kubernetes Secret:</p> <pre><code>spec:\n  containers:\n  - name: maas-api\n    env:\n    - name: TLS_CERT\n      value: /etc/maas-api/tls/tls.crt\n    - name: TLS_KEY\n      value: /etc/maas-api/tls/tls.key\n    volumeMounts:\n    - name: tls-certs\n      mountPath: /etc/maas-api/tls\n      readOnly: true\n  volumes:\n  - name: tls-certs\n    secret:\n      secretName: maas-api-tls-cert\n</code></pre>"},{"location":"configuration-and-management/tls-configuration/#kustomize-overlays","title":"Kustomize Overlays","text":"<p>Pre-configured overlays are available for common scenarios:</p> Overlay Description <code>deployment/base/maas-api/overlays/tls</code> Base TLS overlay for maas-api (deployment patch, service annotation, DestinationRule) <code>deployment/overlays/tls-backend</code> Full TLS deployment with Authorino configuration <code>deployment/overlays/tls-backend-disk</code> TLS + persistent storage (PVC) <code>deployment/overlays/http-backend</code> HTTP only (development/testing) <p>The <code>tls</code> base overlay includes:</p> Resource Purpose <code>deployment-patch.yaml</code> Configure maas-api container for TLS <code>service-patch.yaml</code> Add serving-cert annotation, expose port 8443 <code>destinationrule.yaml</code> Configure gateway TLS to maas-api backend <p>Deploy using:</p> <pre><code>kustomize build deployment/overlays/tls-backend | kubectl apply -f -\n</code></pre>"},{"location":"configuration-and-management/tls-configuration/#verifying-tls-configuration","title":"Verifying TLS Configuration","text":"<p>Application namespace</p> <p>The <code>maas-api</code> service is deployed to the platform's application namespace: <code>redhat-ods-applications</code> for RHOAI or <code>opendatahub</code> for ODH. Adjust the namespace in the commands below accordingly.</p>"},{"location":"configuration-and-management/tls-configuration/#check-certificate","title":"Check Certificate","text":"<pre><code># View certificate details\nkubectl get secret maas-api-serving-cert -n &lt;application-namespace&gt; -o jsonpath='{.data.tls\\.crt}' \\\n  | base64 -d | openssl x509 -text -noout\n\n# Check expiry\nkubectl get secret maas-api-serving-cert -n &lt;application-namespace&gt; -o jsonpath='{.data.tls\\.crt}' \\\n  | base64 -d | openssl x509 -enddate -noout\n</code></pre>"},{"location":"configuration-and-management/tls-configuration/#test-https-endpoint","title":"Test HTTPS Endpoint","text":"<pre><code># From within the cluster\nkubectl run curl --rm -it --image=curlimages/curl -- \\\n  curl -vk https://maas-api.&lt;application-namespace&gt;.svc:8443/health\n\n# Check certificate chain\nopenssl s_client -connect maas-api.&lt;application-namespace&gt;.svc:8443 -servername maas-api.&lt;application-namespace&gt;.svc\n</code></pre>"},{"location":"configuration-and-management/token-management/","title":"Understanding Token Management","text":"<p>This guide explains the token-based authentication system used to access models in the tier-based access control system.  It covers how token issuance works, the underlying service account architecture, and token lifecycle management.</p> <p>Note</p> <p>Prerequisites: This document assumes you have already configured tiers, RBAC, and rate limits.  See Configuring Subscription Tiers for setup instructions.</p>"},{"location":"configuration-and-management/token-management/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>How Token Issuance Works</li> <li>Model Discovery</li> <li>Practical Usage</li> <li>Token Lifecycle Management</li> <li>Frequently Asked Questions (FAQ)</li> <li>Related Documentation</li> </ol>"},{"location":"configuration-and-management/token-management/#overview","title":"Overview","text":"<p>The platform uses a secure, token-based authentication system. Instead of using your primary OpenShift credentials to  access models directly, you first exchange them for a temporary, specialized access token. This approach provides several key benefits:</p> <ul> <li>Enhanced Security: Tokens are short-lived, reducing the risk of compromised credentials. They are also narrowly scoped for model access only.</li> <li>Tier-Based Access Control: The token you receive is automatically associated with your subscription tier (e.g., free, premium), ensuring you get the correct permissions and rate limits.</li> <li>Auditability: Every request made with a token is tied to a specific identity and can be audited.</li> <li>Kubernetes-Native Integration: The system leverages standard, Kubernetes authentication and authorization mechanisms.</li> </ul> <p>The process is simple:</p> <pre><code>You authenticate with OpenShift \u2192 Request a token from the API \u2192 Use that token to call models\n</code></pre>"},{"location":"configuration-and-management/token-management/#how-token-issuance-works","title":"How Token Issuance Works","text":"<p>When you request a token, you are essentially trading your long-term OpenShift identity for a short-term,  purpose-built identity in the form of a Kubernetes Service Account.</p>"},{"location":"configuration-and-management/token-management/#key-concepts","title":"Key Concepts","text":"<ul> <li>Tier Namespace: The platform maintains a separate Kubernetes namespace for each subscription tier (e.g., <code>...-tier-free</code>, <code>...-tier-premium</code>). These namespaces isolate users based on their access level. </li> <li>Service Account (SA): When you request a token for the first time, the system creates a Service Account that represents you inside your designated tier namespace. This SA inherits all the permissions assigned to that tier.</li> <li>Access Token: The token you receive is a standard JSON Web Token (JWT) that authenticates you as that specific Service Account. When you present this token to the gateway, the system knows your identity, your tier, and what permissions you have.</li> <li>Token Audience: The intended recipient of your token. This is validated during authentication and must match the gateway's configuration.</li> <li>Token Expiration: The time after which the token expires. Tokens are short-lived to reduce the risk of compromised credentials.</li> </ul>"},{"location":"configuration-and-management/token-management/#token-issuance-flow","title":"Token Issuance Flow","text":"<p>This diagram illustrates the process of obtaining a token.</p> <pre><code>sequenceDiagram\n    participant User as OpenShift User\n    participant MaaS as maas-api\n    participant K8s as Kubernetes API\n    participant TierNS as Tier Namespace&lt;br/&gt;(e.g., *-tier-premium)\n    participant Gateway\n    participant Model as Model Backend\n\n    Note over User,MaaS: Token Issuance\n    User-&gt;&gt;MaaS: 1. Authenticate with OpenShift token\n    MaaS-&gt;&gt;K8s: Validate token (TokenReview)\n    K8s--&gt;&gt;MaaS: username, groups\n    Note right of MaaS: Determine tier from&lt;br/&gt;tier-to-group-mapping\n    MaaS-&gt;&gt;K8s: Ensure tier namespace exists\n    K8s-&gt;&gt;TierNS: Create if needed\n    MaaS-&gt;&gt;TierNS: Create/get Service Account for user\n    TierNS--&gt;&gt;MaaS: SA ready\n    MaaS-&gt;&gt;K8s: Request SA token (TokenRequest)\n    K8s--&gt;&gt;MaaS: Issued token\n    MaaS--&gt;&gt;User: Return issued token\n\n    Note over User,Model: Model Access\n    User-&gt;&gt;Gateway: 3. Request with issued token\n    Gateway-&gt;&gt;K8s: Validate token (TokenReview)\n    Note right of K8s: Token from SA in&lt;br/&gt;tier namespace\n    K8s--&gt;&gt;Gateway: Valid, with groups\n    Note right of Gateway: Tier lookup,&lt;br/&gt;SAR check,&lt;br/&gt;Rate limits\n    Gateway-&gt;&gt;Model: 4. Authorized request\n    Model--&gt;&gt;Gateway: Response\n    Gateway--&gt;&gt;User: Response</code></pre>"},{"location":"configuration-and-management/token-management/#model-discovery","title":"Model Discovery","text":"<p>The <code>/v1/models</code> endpoint allows you to discover which models you're authorized to access. This endpoint works with any valid authentication token - you don't need to create an API key first.</p>"},{"location":"configuration-and-management/token-management/#how-it-works","title":"How It Works","text":"<p>When you call <code>/v1/models</code>, the system checks your authorization against each model's endpoint. If your token doesn't have the correct audience for these internal checks, the API automatically exchanges it for a short-lived service account token.</p> <pre><code>flowchart LR\n    A[Your Token] --&gt; B{Has correct&lt;br/&gt;audience?}\n    B --&gt;|Yes| D[Use as-is]\n    B --&gt;|No| C[Auto-exchange]\n    C --&gt; D\n    D --&gt; E[Check model&lt;br/&gt;authorization]\n    E --&gt; F[Return&lt;br/&gt;authorized models]</code></pre> <p>This means you can:</p> <ol> <li>Authenticate with OpenShift or OIDC - use your existing identity</li> <li>Call <code>/v1/models</code> immediately - see available models without creating an API key first</li> </ol> <p>Security Note</p> <p>The token exchange is an internal implementation detail. Authentication is still fully handled by Authorino -  the API only peeks at the token's audience claim to decide if exchange is needed. Your identity always comes  from Authorino's validated headers, not from parsing the token.</p>"},{"location":"configuration-and-management/token-management/#practical-usage","title":"Practical Usage","text":"<p>For step-by-step instructions on obtaining and using tokens to access models, including practical examples and troubleshooting, see the Self-Service Model Access Guide.</p> <p>That guide provides:</p> <ul> <li>Complete walkthrough for getting your OpenShift token</li> <li>How to request an access token from the API</li> <li>Examples of making inference requests with your token</li> <li>Troubleshooting common authentication issues</li> </ul>"},{"location":"configuration-and-management/token-management/#token-lifecycle-management","title":"Token Lifecycle Management","text":"<p>Access tokens are ephemeral and must be managed accordingly.</p>"},{"location":"configuration-and-management/token-management/#token-expiration","title":"Token Expiration","text":"<p>Tokens have a finite lifetime for security purposes:</p> <ul> <li>Default lifetime: 4 hours (configurable when requesting)</li> <li>Maximum lifetime: Determined by your Kubernetes cluster configuration</li> </ul> <p>When a token expires, any API request using it will fail with an <code>HTTP 401 Unauthorized error</code>.  To continue, you must request a new token using the process described above.</p> <p>Tips:</p> <ul> <li>For interactive use, request tokens with a lifetime that covers your session (e.g., 4h).</li> <li>For automated scripts or applications, implement logic to refresh the token proactively before it expires.</li> </ul>"},{"location":"configuration-and-management/token-management/#token-revocation","title":"Token Revocation","text":"<p>You can invalidate all active tokens associated with your user account. This is a key security feature if you believe a token has been exposed.</p> <p>To revoke all your tokens, send a <code>DELETE</code> request to the <code>/v1/tokens</code> endpoint.</p> <p><pre><code>curl -sSk -X DELETE \"${MAAS_API_URL}/v1/tokens\" \\\n  -H \"Authorization: Bearer $(oc whoami -t)\"\n</code></pre> This action immediately deletes your underlying Service Account, which invalidates all tokens that have ever been issued for it.  The Service Account will be automatically recreated the next time you request a token.</p> <p>Important</p> <p>For Platform Administrators: Admins can manually revoke a user's tokens by finding and deleting their Service Account  in the appropriate tier namespace (e.g., <code>&lt;instance-name&gt;-tier-premium</code>). This is an effective way to immediately cut  off access for a specific user in response to a security event.</p>"},{"location":"configuration-and-management/token-management/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":"<p>Q: My tier is wrong or shows as \"free\". How do I fix it?</p> <p>A: Your tier is determined by your group membership in OpenShift. Contact your platform administrator to ensure you  are in the correct user group, which should be mapped to your desired tier in the tier mapping configuration.</p> <p>Q: How long should my tokens be valid for?</p> <p>A: It's a balance of security and convenience. For interactive command-line use, 1-8 hours is common. For applications, request shorter-lived tokens (e.g., 15-60 minutes) and refresh them automatically.</p> <p>Q: Can I have multiple active tokens at once?</p> <p>A: Yes. Each call to the <code>/v1/tokens</code> endpoint issues a new, independent token. All of them will be valid until they expire or are revoked.</p> <p>Q: What happens if the <code>maas-api</code> service is down?</p> <p>A: You will not be able to issue new tokens. However, any existing, non-expired tokens will continue to work for calling models, as the gateway validates them directly with the Kubernetes API.</p> <p>Q: Can I use one token to access multiple different models?</p> <p>A: Yes. Your token grants you access based on your tier's RBAC permissions. If your tier is authorized to use multiple models, a single token will work for all of them.</p> <p>Q: What's the difference between my OpenShift token and an API key?</p> <p>A: Your OpenShift token is your identity token from authentication (e.g. OpenShift or OIDC). It proves who you are but may not have the correct audience for model access. An API key (issued via <code>/v1/tokens</code>) is a service account token with the correct audience and permissions for accessing models. You can use either to list models, but only API keys work for model inference.</p> <p>Q: Do I need an API key to list available models?</p> <p>A: No. You can call <code>/v1/models</code> with your OpenShift/OIDC token directly. The API automatically handles token exchange if needed. This lets you discover available models before deciding which API keys to create.</p> <p>Q: What is \"token audience\" and why does it matter?</p> <p>A: Token audience identifies the intended recipient of a token. Model endpoints expect tokens with a specific audience (<code>{tenant}-sa</code>). OpenShift/OIDC tokens have different audiences, so the API exchanges them internally when needed for authorization checks.</p>"},{"location":"configuration-and-management/token-management/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuring Subscription Tiers: For operators - tier setup, RBAC, and rate limiting configuration</li> </ul>"},{"location":"install/maas-setup/","title":"Install MaaS Components","text":"<p>After enabling MaaS in your DataScienceCluster (set <code>modelsAsService.managementState: Managed</code> in the <code>spec.components.kserve</code> section - see platform setup guide for the complete configuration), the operator will automatically deploy:</p> <ul> <li>MaaS API (Deployment, Service, ServiceAccount, ClusterRole, ClusterRoleBinding, HTTPRoute)</li> <li>MaaS API AuthPolicy (maas-api-auth-policy) - Protects the MaaS API endpoint</li> <li>NetworkPolicy (maas-authorino-allow) - Allows Authorino to reach MaaS API</li> </ul> <p>You must manually install the following components after completing the platform setup (which includes creating the required <code>maas-default-gateway</code>):</p> <p>The tools you will need:</p> <ul> <li><code>kubectl</code> or <code>oc</code> client (this guide uses <code>kubectl</code>)</li> <li><code>kustomize</code></li> <li><code>envsubst</code></li> </ul>"},{"location":"install/maas-setup/#install-gateway-authpolicy","title":"Install Gateway AuthPolicy","text":"<p>Install the authentication policy for the Gateway. This policy applies to model inference traffic and integrates with the MaaS API for tier-based access control:</p> <pre><code># For RHOAI installations (MaaS API in redhat-ods-applications namespace)\nkubectl apply --server-side=true \\\n  -f &lt;(kustomize build \"https://github.com/opendatahub-io/models-as-a-service.git/deployment/base/policies/auth-policies?ref=main\" | \\\n       sed \"s/maas-api\\.maas-api\\.svc/maas-api.redhat-ods-applications.svc/g\")\n\n# For ODH installations (MaaS API in opendatahub namespace)\nkubectl apply --server-side=true \\\n  -f &lt;(kustomize build \"https://github.com/opendatahub-io/models-as-a-service.git/deployment/base/policies/auth-policies?ref=main\" | \\\n       sed \"s/maas-api\\.maas-api\\.svc/maas-api.opendatahub.svc/g\")\n</code></pre> <p>Custom Token Review Audience</p> <p>If you encounter <code>401 Unauthorized</code> errors when obtaining tokens, your cluster may use a custom token review audience. See Troubleshooting - 401 Errors for detection and resolution steps.</p>"},{"location":"install/maas-setup/#install-usage-policies","title":"Install Usage Policies","text":"<p>Install rate limiting policies (TokenRateLimitPolicy and RateLimitPolicy):</p> <pre><code>export CLUSTER_DOMAIN=$(kubectl get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}')\n\nkubectl apply --server-side=true \\\n  -f &lt;(kustomize build \"https://github.com/opendatahub-io/models-as-a-service.git/deployment/base/policies/usage-policies?ref=main\" | \\\n       envsubst '$CLUSTER_DOMAIN')\n</code></pre> <p>These policies define:</p> <ul> <li>TokenRateLimitPolicy - Rate limits based on token consumption per tier</li> <li>RateLimitPolicy - Request rate limits per tier</li> </ul> <p>See Tier Management for more details on configuring usage policies and tiers.</p>"},{"location":"install/maas-setup/#next-steps","title":"Next steps","text":"<ul> <li>Deploy models. See the Quick Start for   sample model deployments that you   can use to try the MaaS capability.</li> <li>Perform validation. Follow the validation guide to verify that   MaaS is working correctly.</li> </ul>"},{"location":"install/platform-setup/","title":"Install Open Data Hub or Red Hat OpenShift AI","text":"<p>This guide covers the installation of either Open Data Hub (ODH) or Red Hat OpenShift AI (RHOAI), with the required configuration to enable the Models-as-a-Service capability (MaaS).</p> <p>Choose Your Platform</p> <p>You should choose either Open Data Hub or Red Hat OpenShift AI - do not install both. The installation steps are similar with a few platform-specific differences noted throughout.</p>"},{"location":"install/platform-setup/#prerequisites","title":"Prerequisites","text":"<p>You need a Red Hat OpenShift cluster version 4.19.9 or later. Older OpenShift versions are not suitable.</p> <p>MaaS requires the Model Serving component configured for deploying models with <code>LLMInferenceService</code> resources. The prerequisites for this setup are a Gateway API controller (Kuadrant or RHCL) and the LeaderWorkerSet API (LWS).</p> <p>Tools you will need:</p> <ul> <li><code>kubectl</code> or <code>oc</code> client (this guide uses <code>kubectl</code>)</li> </ul> <p>For ODH installations only:</p> <ul> <li><code>curl</code></li> <li><code>jq</code></li> </ul> <p>Documentation References</p> <p>This guide is provided for convenience. In case of any issues or more advanced setups:</p> <ul> <li>ODH: Refer to Kuadrant documentation</li> <li>RHOAI: Refer to Red Hat documentation</li> </ul>"},{"location":"install/platform-setup/#install-leaderworkerset-api","title":"Install LeaderWorkerSet API","text":"Red Hat OpenShift AIOpen Data Hub <p>Install Red Hat LeaderWorkerSet API (LWS) Operator from OpenShift's built-in OperatorHub:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-lws-operator\n---\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: leader-worker-set\n  namespace: openshift-lws-operator\nspec:\n  targetNamespaces:\n  - openshift-lws-operator\n---\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: leader-worker-set\n  namespace: openshift-lws-operator\nspec:\n  channel: stable-v1.0\n  installPlanApproval: Automatic\n  name: leader-worker-set\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n</code></pre> <p>Once the LWS operator is ready, set up the LWS API:</p> <pre><code>apiVersion: operator.openshift.io/v1\nkind: LeaderWorkerSetOperator\nmetadata:\n  name: cluster\n  namespace: openshift-lws-operator\nspec:\n  managementState: Managed\n</code></pre> <p>Check Red Hat LWS documentation if you need further guidance.</p> <p>Install the latest version of LWS by using the kubectl method from LWS official documentation:</p> <pre><code>GH_LATEST_LWS_ENTRY_URL=\"https://api.github.com/repos/kubernetes-sigs/lws/releases\"\nLATEST_LWS_VERSION=$(curl -sSf ${GH_LATEST_LWS_ENTRY_URL} | jq -r 'sort_by(.tag_name|ltrimstr(\"v\")|split(\".\")|map(tonumber)) | last | .tag_name')\n\nkubectl apply --server-side -f https://github.com/kubernetes-sigs/lws/releases/download/${LATEST_LWS_VERSION}/manifests.yaml\n</code></pre>"},{"location":"install/platform-setup/#verification","title":"Verification","text":"<p>Check that LWS deployments are ready:</p> Red Hat OpenShift AIOpen Data Hub <pre><code>kubectl get deployments --namespace openshift-lws-operator\n\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\nlws-controller-manager   2/2     2            2           61s\nopenshift-lws-operator   1/1     1            1           4m26s\n</code></pre> <pre><code>kubectl get deployments --namespace lws-system\n\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\nlws-controller-manager   2/2     2            2           35s\n</code></pre>"},{"location":"install/platform-setup/#install-gateway-api-controller","title":"Install Gateway API Controller","text":"<p>Initialize OpenShift's provided Gateway API implementation:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: openshift-default\nspec:\n  controllerName: \"openshift.io/gateway-controller/v1\"\n</code></pre> <p>Wait until the GatewayClass resource is accepted:</p> <pre><code>kubectl get gatewayclass openshift-default\n\nNAME                CONTROLLER                           ACCEPTED   AGE\nopenshift-default   openshift.io/gateway-controller/v1   True       52s\n</code></pre> <p>Now install the Gateway API controller for your platform:</p> Red Hat OpenShift AIOpen Data Hub <p>Install Red Hat Connectivity Link (RHCL) Operator from OpenShift's built-in OperatorHub. MaaS requires RHCL v1.2 or later:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: kuadrant-system\n---\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: kuadrant-operator-group\n  namespace: kuadrant-system\n---\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: kuadrant-operator\n  namespace: kuadrant-system\nspec:\n  channel: stable\n  installPlanApproval: Automatic\n  name: rhcl-operator\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n</code></pre> <p>Once the RHCL operator is ready, create a Connectivity Link instance:</p> <pre><code>apiVersion: kuadrant.io/v1beta1\nkind: Kuadrant\nmetadata:\n  name: kuadrant\n  namespace: kuadrant-system\n</code></pre> <p>Install Kuadrant using the OLM method. MaaS requires Kuadrant v1.3.1 or later.</p> <p>Create the <code>kuadrant-system</code> namespace:</p> <pre><code>kubectl create namespace kuadrant-system\n</code></pre> <p>Create an OperatorGroup:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: kuadrant-operator-group\n  namespace: kuadrant-system\nspec: {}\nEOF\n</code></pre> <p>Note</p> <p>A single OperatorGroup should exist in any given namespace. Check for the existence of multiple OperatorGroups if Kuadrant operator is not deployed successfully.</p> <p>Configure Kuadrant's CatalogSource:</p> <pre><code># Find latest Kuadrant operator version:\nGH_LATEST_KUADRANT_ENTRY_URL=\"https://api.github.com/repos/Kuadrant/kuadrant-operator/releases/latest\"\nLATEST_KUADRANT_VERSION=$(curl -sSf ${GH_LATEST_KUADRANT_ENTRY_URL} | jq -r '.tag_name')\n\n# Install the CatalogSource\nkubectl apply -f - &lt;&lt;EOF\napiVersion: operators.coreos.com/v1alpha1\nkind: CatalogSource\nmetadata:\n  name: kuadrant-operator-catalog\n  namespace: kuadrant-system\nspec:\n  displayName: Kuadrant Operators\n  image: quay.io/kuadrant/kuadrant-operator-catalog:${LATEST_KUADRANT_VERSION}\n  sourceType: grpc\nEOF\n</code></pre> <p>Deploy the Kuadrant operator, configuring it to work with OpenShift's Gateway API:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: kuadrant-operator\n  namespace: kuadrant-system\nspec:\n  channel: stable\n  installPlanApproval: Automatic\n  name: kuadrant-operator\n  source: kuadrant-operator-catalog\n  sourceNamespace: kuadrant-system\n  config:\n    env:\n    - name: \"ISTIO_GATEWAY_CONTROLLER_NAMES\"\n      value: \"openshift.io/gateway-controller/v1\"\nEOF\n</code></pre> <p>Once the Kuadrant operator is ready, create a Kuadrant instance:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: kuadrant.io/v1beta1\nkind: Kuadrant\nmetadata:\n  name: kuadrant\n  namespace: kuadrant-system\nEOF\n</code></pre>"},{"location":"install/platform-setup/#verification_1","title":"Verification","text":"<p>Check that Gateway API controller deployments are ready:</p> <pre><code>kubectl get deployments -n kuadrant-system\n\nNAME                                    READY   UP-TO-DATE   AVAILABLE   AGE\nauthorino-operator                      1/1     1            1           80s\ndns-operator-controller-manager         1/1     1            1           77s\nkuadrant-console-plugin                 1/1     1            1           58s\nkuadrant-operator-controller-manager    1/1     1            1           69s\nlimitador-operator-controller-manager   1/1     1            1           73s\n</code></pre> <p>For RHOAI installations, you should also see:</p> <pre><code>authorino                               1/1     1            1           81s\nlimitador-limitador                     1/1     1            1           82s\n</code></pre>"},{"location":"install/platform-setup/#install-platform-with-model-serving","title":"Install Platform with Model Serving","text":"<p>First, set up the inference Gateway required by Model Serving:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: openshift-ai-inference\n  namespace: openshift-ingress\nspec:\n  gatewayClassName: openshift-default\n  listeners:\n  - name: http\n    port: 80\n    protocol: HTTP\n    allowedRoutes:\n      namespaces:\n        from: All\n  infrastructure:\n    labels:\n      serving.kserve.io/gateway: kserve-ingress-gateway\n</code></pre> <p>Gateway Architecture</p> <p>MaaS uses a segregated gateway approach where models explicitly opt-in to MaaS capabilities. The <code>openshift-ai-inference</code> gateway above is for standard KServe inference, while <code>maas-default-gateway</code> (created later) enables token authentication and rate limiting. For details, see Model Setup - Gateway Architecture.</p> <p>Now install the platform operator for your environment:</p> Red Hat OpenShift AIOpen Data Hub <p>Install Red Hat OpenShift AI (RHOAI) Operator from OpenShift's built-in OperatorHub. MaaS requires RHOAI v3.0 or later:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: redhat-ods-operator\n---\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: rhoai3-operatorgroup\n  namespace: redhat-ods-operator\n---\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: rhoai3-operator\n  namespace: redhat-ods-operator\nspec:\n  channel: fast-3.x\n  installPlanApproval: Automatic\n  name: rhods-operator\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n</code></pre> <p>Once ready, the RHOAI Operator should automatically create a <code>DSCInitialization</code> resource. Install the Model Serving component by creating the following <code>DataScienceCluster</code> resource:</p> <pre><code>apiVersion: datasciencecluster.opendatahub.io/v2\nkind: DataScienceCluster\nmetadata:\n  name: default-dsc\nspec:\n  components:\n    # Components required for MaaS:\n    kserve:\n      managementState: Managed\n      rawDeploymentServiceConfig: Headed\n      # Enable Models-as-a-Service via operator\n      modelsAsService:\n        managementState: Managed\n\n    # Components recommended for MaaS:\n    dashboard:\n      managementState: Managed\n</code></pre> <p>Install the Open Data Hub Project (ODH) operator, which is available in OpenShift's preconfigured CatalogSource of community operators. MaaS requires ODH v3.0 or later:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: opendatahub-operator\n  namespace: openshift-operators\nspec:\n  channel: fast-3\n  name: opendatahub-operator\n  source: community-operators\n  sourceNamespace: openshift-marketplace\nEOF\n</code></pre> <p>Install the ODH Model Serving component by creating two resources:</p> <ol> <li>A <code>DSCInitialization</code> resource to initialize the ODH platform</li> <li>A <code>DataScienceCluster</code> resource to install ODH components</li> </ol> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: dscinitialization.opendatahub.io/v2\nkind: DSCInitialization\nmetadata:\n  name: default-dsci\nspec:\n  applicationsNamespace: opendatahub\n  monitoring:\n    managementState: Managed\n    namespace: opendatahub\n    metrics: {}\n  trustedCABundle:\n    managementState: Managed\n---\napiVersion: datasciencecluster.opendatahub.io/v2\nkind: DataScienceCluster\nmetadata:\n  name: default-dsc\nspec:\n  components:\n    # Components required for MaaS:\n    kserve:\n      managementState: Managed\n      rawDeploymentServiceConfig: Headed\n      # Enable Models-as-a-Service via operator\n      modelsAsService:\n        managementState: Managed\n\n    # Components recommended for MaaS:\n    dashboard:\n      managementState: Managed\nEOF\n</code></pre> <p>MaaS via Operator</p> <p>When <code>modelsAsService.managementState</code> is set to <code>Managed</code>, the operator will deploy the MaaS API, MaaS API AuthPolicy, and NetworkPolicy automatically. However, the Gateway, Gateway AuthPolicy, TokenRateLimitPolicy, and RateLimitPolicy must still be installed manually following the instructions below and in maas-setup.md.</p>"},{"location":"install/platform-setup/#create-maas-gateway","title":"Create MaaS Gateway","text":"<p>A Gateway with the name <code>maas-default-gateway</code> is required for MaaS to function. The configuration below provides an example Gateway you can use:</p> <p>Example Gateway Configuration</p> <p>The Gateway configuration below is provided as an example. Depending on your cluster setup, you may need additional configuration such as TLS certificates, specific listener settings, or custom infrastructure labels. Consult your cluster administrator if you're unsure about Gateway requirements for your environment.</p> <pre><code># Get your cluster's domain\nCLUSTER_DOMAIN=$(kubectl get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}')\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: maas-default-gateway\n  namespace: openshift-ingress\nspec:\n  gatewayClassName: openshift-default\n  listeners:\n   - name: http\n     hostname: maas.${CLUSTER_DOMAIN}\n     port: 80\n     protocol: HTTP\n     allowedRoutes:\n       namespaces:\n         from: All\nEOF\n</code></pre> <p>Wait for the Gateway to be programmed:</p> <pre><code>kubectl wait --for=condition=Programmed gateway/maas-default-gateway -n openshift-ingress --timeout=60s\n</code></pre> <p>Note</p> <p>The <code>maas-default-gateway</code> created above satisfies the Gateway requirement for MaaS. When following the next steps, you can skip the Gateway creation and proceed directly to installing the Gateway AuthPolicy and usage policies in maas-setup.md.</p>"},{"location":"install/platform-setup/#verification_2","title":"Verification","text":"<p>Check that all MaaS components are running:</p> Red Hat OpenShift AIOpen Data Hub <pre><code># Check RHOAI Model Serving deployments\nkubectl get deployments -n redhat-ods-applications\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\nkserve-controller-manager   1/1     1            1           73s\nodh-model-controller        1/1     1            1           79s\nrhods-dashboard             2/2     2            2           78s\nmaas-api                    1/1     1            1           60s  # Only if MaaS enabled\n</code></pre> <pre><code># Check MaaS API deployment\nkubectl get deployment maas-api -n opendatahub\n\n# Check HTTPRoute\nkubectl get httproute maas-api-route -n opendatahub\n\n# Check AuthPolicy\nkubectl get authpolicy maas-api-auth-policy -n opendatahub\n\n# Check NetworkPolicy (allows Authorino to reach MaaS API)\nkubectl get networkpolicy maas-authorino-allow -n opendatahub\n</code></pre> <p>All resources should exist and the MaaS API deployment should show <code>READY 1/1</code>.</p>"},{"location":"install/platform-setup/#test-maas-api-connectivity","title":"Test MaaS API Connectivity","text":"<p>Verify that Authorino can communicate with the MaaS API:</p> Red Hat OpenShift AIOpen Data Hub <pre><code># Get Authorino pod\nAUTHORINO_POD=$(kubectl get pods -n kuadrant-system -l authorino-resource=authorino -o jsonpath='{.items[0].metadata.name}')\n\n# Test connectivity\nkubectl exec -n kuadrant-system $AUTHORINO_POD -- curl -s \\\n  http://maas-api.redhat-ods-applications.svc.cluster.local:8080/health\n</code></pre> <pre><code># Get Authorino pod\nAUTHORINO_POD=$(kubectl get pods -n kuadrant-system -l authorino-resource=authorino -o jsonpath='{.items[0].metadata.name}')\n\n# Test connectivity\nkubectl exec -n kuadrant-system $AUTHORINO_POD -- curl -s \\\n  http://maas-api.opendatahub.svc.cluster.local:8080/health\n</code></pre> <p>Expected output:</p> <pre><code>{\"status\":\"healthy\"}\n</code></pre> <p>For end-to-end validation and troubleshooting, see the Validation Guide.</p>"},{"location":"install/platform-setup/#next-steps","title":"Next Steps","text":"<p>Once your platform with MaaS is installed:</p> <ol> <li>Install MaaS Components - Install Gateway AuthPolicy and usage policies</li> <li>Deploy a Model - Deploy your first LLMInferenceService</li> </ol>"},{"location":"install/prerequisites/","title":"MaaS Installation Overview","text":"<p>ODH's Models-as-a-Service is compatible with the Open Data Hub project (ODH) and Red Hat OpenShift AI (RHOAI). MaaS is installed by enabling it in the DataScienceCluster resource:</p> <ul> <li>Install your platform (ODH or RHOAI) with MaaS enabled in the DataScienceCluster   (the operator will automatically deploy the MaaS API, MaaS API AuthPolicy, and NetworkPolicy).</li> <li>Install Gateway and policies manually (Gateway, Gateway AuthPolicy, TokenRateLimitPolicy, and RateLimitPolicy).</li> </ul> <p>MaaS inherits the platform requirement for a Red Hat OpenShift cluster version 4.19.9 or later, which is the version that has formal support for Gateway API. For earlier OpenShift versions, there are alternatives (e.g. see a guide here), but we provide no support for such setups.</p>"},{"location":"install/prerequisites/#requirements-for-open-data-hub-project","title":"Requirements for Open Data Hub project","text":"<p>MaaS requires Open Data Hub version 3.0 or later, with the Model Serving component enabled (KServe) and properly configured for deploying models with <code>LLMInferenceService</code> resources.</p> <p>A specific requirement for MaaS is to set up ODH's Model Serving with Kuadrant v1.3+, even though ODH can work with earlier Kuadrant versions.</p>"},{"location":"install/prerequisites/#requirements-for-red-hat-openshift-ai","title":"Requirements for Red Hat OpenShift AI","text":"<p>MaaS requires Red Hat OpenShift AI (RHOAI) version 3.0 or later, with the Model Serving component enabled (KServe) and properly configured for deploying models with <code>LLMInferenceService</code> resources.</p> <p>A specific requirement for MaaS is to set up RHOAI Model Serving with Red Hat Connectivity Link (RHCL) v1.2+, even though RHOAI can work with earlier RHCL versions.</p>"},{"location":"install/validation/","title":"Validation Guide","text":"<p>This guide provides instructions for validating and testing your MaaS Platform deployment.</p>"},{"location":"install/validation/#namespace-reference","title":"Namespace Reference","text":"Component RHOAI ODH MaaS API redhat-ods-applications opendatahub Kuadrant/RHCL kuadrant-system kuadrant-system Gateway openshift-ingress openshift-ingress"},{"location":"install/validation/#manual-validation-recommended","title":"Manual Validation (Recommended)","text":"<p>Follow these steps to validate your deployment and understand each component:</p>"},{"location":"install/validation/#1-get-gateway-endpoint","title":"1. Get Gateway Endpoint","text":"<pre><code>CLUSTER_DOMAIN=$(kubectl get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}') &amp;&amp; \\\nHOST=\"https://maas.${CLUSTER_DOMAIN}\" &amp;&amp; \\\necho \"Gateway endpoint: $HOST\"\n</code></pre> <p>Note</p> <p>If you haven't created the <code>maas-default-gateway</code> yet, you can use the fallback: <pre><code>HOST=\"https://gateway.${CLUSTER_DOMAIN}\" &amp;&amp; \\\necho \"Using fallback gateway endpoint: $HOST\"\n</code></pre></p>"},{"location":"install/validation/#2-get-authentication-token","title":"2. Get Authentication Token","text":"<p>For OpenShift:</p> <pre><code>TOKEN_RESPONSE=$(curl -sSk \\\n  -H \"Authorization: Bearer $(oc whoami -t)\" \\\n  -H \"Content-Type: application/json\" \\\n  -X POST \\\n  -d '{\"expiration\": \"10m\"}' \\\n  \"${HOST}/maas-api/v1/tokens\") &amp;&amp; \\\nTOKEN=$(echo $TOKEN_RESPONSE | jq -r .token) &amp;&amp; \\\necho \"Token obtained: ${TOKEN:0:20}...\"\n</code></pre> <p>Note</p> <p>For more information about how tokens work, see Understanding Token Management.</p>"},{"location":"install/validation/#3-list-available-models","title":"3. List Available Models","text":"<pre><code>MODELS=$(curl -sSk ${HOST}/maas-api/v1/models \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $TOKEN\" | jq -r .) &amp;&amp; \\\necho $MODELS | jq . &amp;&amp; \\\nMODEL_NAME=$(echo $MODELS | jq -r '.data[0].id') &amp;&amp; \\\nMODEL_URL=$(echo $MODELS | jq -r '.data[0].url') &amp;&amp; \\\necho \"Model URL: $MODEL_URL\"\n</code></pre>"},{"location":"install/validation/#4-test-model-inference-endpoint","title":"4. Test Model Inference Endpoint","text":"<p>Send a request to the model endpoint (should get a 200 OK response):</p> <pre><code>curl -sSk -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"model\\\": \\\"${MODEL_NAME}\\\", \\\"prompt\\\": \\\"Hello\\\", \\\"max_tokens\\\": 50}\" \\\n  \"${MODEL_URL}/v1/completions\" | jq\n</code></pre>"},{"location":"install/validation/#5-test-authorization-enforcement","title":"5. Test Authorization Enforcement","text":"<p>Send a request to the model endpoint without a token (should get a 401 Unauthorized response):</p> <pre><code>curl -sSk -H \"Content-Type: application/json\" \\\n  -d \"{\\\"model\\\": \\\"${MODEL_NAME}\\\", \\\"prompt\\\": \\\"Hello\\\", \\\"max_tokens\\\": 50}\" \\\n  \"${MODEL_URL}/v1/completions\" -v\n</code></pre>"},{"location":"install/validation/#6-test-rate-limiting","title":"6. Test Rate Limiting","text":"<p>Send multiple requests to trigger rate limit (should get 200 OK followed by 429 Rate Limit Exceeded after about 4 requests):</p> <pre><code>for i in {1..16}; do\n  curl -sSk -o /dev/null -w \"%{http_code}\\n\" \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\\\"model\\\": \\\"${MODEL_NAME}\\\", \\\"prompt\\\": \\\"Hello\\\", \\\"max_tokens\\\": 50}\" \\\n    \"${MODEL_URL}/v1/completions\"\ndone\n</code></pre>"},{"location":"install/validation/#7-verify-component-status","title":"7. Verify Component Status","text":"<p>Check that all components are running:</p> <pre><code>kubectl get pods -n maas-api &amp;&amp; \\\nkubectl get pods -n kuadrant-system &amp;&amp; \\\nkubectl get pods -n kserve &amp;&amp; \\\nkubectl get pods -n llm\n</code></pre> <p>Check Gateway status:</p> <pre><code>kubectl get gateway -n openshift-ingress maas-default-gateway\n</code></pre> <p>Check that policies are enforced:</p> <pre><code>kubectl get authpolicy -A &amp;&amp; \\\nkubectl get tokenratelimitpolicy -A &amp;&amp; \\\nkubectl get llminferenceservices -n llm\n</code></pre> <p>See the deployment scripts documentation at <code>scripts/README.md</code> for more information about validation and troubleshooting.</p>"},{"location":"install/validation/#automated-validation","title":"Automated Validation","text":"<p>For faster validation, you can use the automated validation script to run the manual validation steps more quickly:</p> <pre><code>./scripts/validate-deployment.sh\n</code></pre> <p>The script automates the manual validation steps above and provides detailed feedback with specific suggestions for fixing any issues found. This is useful when you need to quickly verify deployment status, but understanding the manual steps above helps with troubleshooting.</p>"},{"location":"install/validation/#tls-verification","title":"TLS Verification","text":"<p>TLS is enabled by default when deploying via the automated script or ODH overlay.</p>"},{"location":"install/validation/#check-certificate","title":"Check Certificate","text":"<pre><code># View certificate details (RHOAI)\nkubectl get secret maas-api-serving-cert -n redhat-ods-applications \\\n  -o jsonpath='{.data.tls\\.crt}' | base64 -d | openssl x509 -text -noout\n\n# Check expiry\nkubectl get secret maas-api-serving-cert -n redhat-ods-applications \\\n  -o jsonpath='{.data.tls\\.crt}' | base64 -d | openssl x509 -enddate -noout\n</code></pre>"},{"location":"install/validation/#test-https-endpoint","title":"Test HTTPS Endpoint","text":"<pre><code>kubectl run curl --rm -it --image=curlimages/curl -- \\\n  curl -vk https://maas-api.redhat-ods-applications.svc:8443/health\n</code></pre> <p>For detailed TLS configuration options, see TLS Configuration.</p>"},{"location":"install/validation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"install/validation/#common-issues","title":"Common Issues","text":"<ol> <li>Getting <code>501</code> Not Implemented errors: Traffic is not making it to the Gateway.<ul> <li> Verify Gateway status and HTTPRoute configuration</li> </ul> </li> <li> <p>Getting <code>401</code> Unauthorized errors when trying to get a token: Authentication maas-api is not working.</p> <ul> <li> Verify <code>maas-api-auth-policy</code> AuthPolicy is applied</li> <li> Check if your cluster uses a custom token review audience:</li> </ul> <pre><code># Detect your cluster's audience\nAUD=\"$(kubectl create token default --duration=10m 2&gt;/dev/null | \\\n  cut -d. -f2 | jq -Rr '@base64d | fromjson | .aud[0]' 2&gt;/dev/null)\"\necho \"Cluster audience: ${AUD}\"\n</code></pre> <p>If the audience is NOT <code>https://kubernetes.default.svc</code>, patch the AuthPolicy:</p> <pre><code># For RHOAI:\nkubectl patch authpolicy maas-api-auth-policy -n redhat-ods-applications \\\n  --type=merge --patch \"\nspec:\n  rules:\n    authentication:\n      openshift-identities:\n        kubernetesTokenReview:\n          audiences:\n            - ${AUD}\n            - maas-default-gateway-sa\"\n</code></pre> <p>For ODH, use namespace <code>opendatahub</code> instead of <code>redhat-ods-applications</code>. 3. Getting <code>401</code> errors when trying to get models: Authentication is not working for the models endpoint.   - [ ] Create a new token (default expiration is 10 minutes)   - [ ] Verify <code>gateway-auth-policy</code> AuthPolicy is applied   - [ ] Validate that <code>system:serviceaccounts:maas-default-gateway-tier-{TIER}</code> has <code>post</code> access to the <code>llminferenceservices</code> resource     - Note: this should be automated by the ODH Controller 4. Getting <code>404</code> errors when trying to get models: The models endpoint is not working.   - [ ] Verify <code>model-route</code> HTTPRoute exist and is applied   - [ ] Verify the model is deployed and the <code>LLMInferenceService</code> has the <code>maas-default-gateway</code> gateway specified   - [ ] Verify that the model is recognized by maas-api by checking the <code>maas-api/v1/models</code> endpoint (see List Available Models) 5. Rate limiting not working: Verify AuthPolicy and TokenRateLimitPolicy are applied   - [ ] Verify <code>gateway-rate-limits</code> RateLimitPolicy is applied   - [ ] Verify <code>gateway-token-rate-limits</code> TokenRateLimitPolicy is applied   - [ ] Verify the model is deployed and the <code>LLMInferenceService</code> has the <code>maas-default-gateway</code> gateway specified   - [ ] Verify that the model is rate limited by checking the inference endpoint (see Test Rate Limiting)   - [ ] Verify that the model is token rate limited by checking the inference endpoint (see Test Rate Limiting) 6. Routes not accessible (503 errors): Check MaaS Default Gateway status and HTTPRoute configuration   - [ ] Verify Gateway is in <code>Programmed</code> state: <code>kubectl get gateway -n openshift-ingress maas-default-gateway</code>   - [ ] Check HTTPRoute configuration and status</p> </li> </ol>"},{"location":"user-guide/self-service-model-access/","title":"Self-Service Model Access","text":"<p>This guide is for end users who want to use AI models through the MaaS platform.</p>"},{"location":"user-guide/self-service-model-access/#what-is-maas","title":"\ud83c\udfaf What is MaaS?","text":"<p>The Models-as-a-Service (MaaS) platform provides access to AI models through a simple API. Your organization's administrator has set up the platform and configured access for your team.</p>"},{"location":"user-guide/self-service-model-access/#getting-your-access-token","title":"Getting Your Access Token","text":"<p>Tip</p> <p>For a detailed explanation of how token authentication works, including the underlying service account architecture and security model, see Understanding Token Management.</p>"},{"location":"user-guide/self-service-model-access/#step-1-get-your-openshift-authentication-token","title":"Step 1: Get Your OpenShift Authentication Token","text":"<p>First, you need your OpenShift token to prove your identity to the maas-api.</p> <pre><code># Log in to your OpenShift cluster if you haven't already\noc login ...\n\n# Get your current OpenShift authentication token\nOC_TOKEN=$(oc whoami -t)\n</code></pre>"},{"location":"user-guide/self-service-model-access/#step-2-request-an-access-token-from-the-api","title":"Step 2: Request an Access Token from the API","text":"<p>Next, use that OpenShift token to call the maas-api <code>/v1/tokens</code> endpoint. You can specify the desired expiration time; the default is 4 hours.</p> <pre><code>CLUSTER_DOMAIN=$(kubectl get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}')\nMAAS_API_URL=\"https://maas.${CLUSTER_DOMAIN}\"\n\nTOKEN_RESPONSE=$(curl -sSk \\\n  -H \"Authorization: Bearer ${OC_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -X POST \\\n  -d '{\"expiration\": \"15m\"}' \\\n  \"${MAAS_API_URL}/maas-api/v1/tokens\")\n\nACCESS_TOKEN=$(echo $TOKEN_RESPONSE | jq -r .token)\n\necho $ACCESS_TOKEN\n</code></pre>"},{"location":"user-guide/self-service-model-access/#token-lifecycle","title":"Token Lifecycle","text":"<ul> <li>Default lifetime: 4 hours (configurable when requesting)</li> <li>Maximum lifetime: Determined by cluster configuration</li> <li>Refresh: Request a new token before expiration</li> <li>Revocation: Tokens can be revoked if compromised</li> </ul>"},{"location":"user-guide/self-service-model-access/#discovering-models","title":"Discovering Models","text":""},{"location":"user-guide/self-service-model-access/#list-available-models","title":"List Available Models","text":"<p>Get a list of models available to your tier:</p> <pre><code>MODELS=$(curl \"${MAAS_API_URL}/v1/models\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer ${ACCESS_TOKEN}\")\n\necho $MODELS | jq .\n</code></pre> <p>Example response:</p> <pre><code>{\n  \"data\": [\n    {\n      \"id\": \"simulator\",\n      \"name\": \"Simulator Model\",\n      \"url\": \"https://gateway.your-domain.com/simulator/v1/chat/completions\",\n      \"tier\": \"free\"\n    },\n    {\n      \"id\": \"qwen3\",\n      \"name\": \"Qwen3 Model\",\n      \"url\": \"https://gateway.your-domain.com/qwen3/v1/chat/completions\",\n      \"tier\": \"premium\"\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/self-service-model-access/#get-model-details","title":"Get Model Details","text":"<p>Get detailed information about a specific model:</p> <pre><code>MODEL_ID=\"simulator\"\nMODEL_INFO=$(curl \"${MAAS_API_URL}/v1/models\" \\\n    -H \"Authorization: Bearer ${ACCESS_TOKEN}\" | \\\n    jq --arg model \"$MODEL_ID\" '.data[] | select(.id == $model)')\n\necho $MODEL_INFO | jq .\n</code></pre>"},{"location":"user-guide/self-service-model-access/#making-inference-requests","title":"Making Inference Requests","text":""},{"location":"user-guide/self-service-model-access/#basic-chat-completion","title":"Basic Chat Completion","text":"<p>Make a simple chat completion request:</p> <pre><code># First, get the model URL from the models endpoint\nMODELS=$(curl \"${MAAS_API_URL}/v1/models\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer ${ACCESS_TOKEN}\")\nMODEL_URL=$(echo $MODELS | jq -r '.data[0].url')\nMODEL_NAME=$(echo $MODELS | jq -r '.data[0].id')\n\ncurl -sSk \\\n  -H \"Authorization: Bearer ${ACCESS_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n        \\\"model\\\": \\\"${MODEL_NAME}\\\",\n        \\\"messages\\\": [\n          {\n            \\\"role\\\": \\\"user\\\",\n            \\\"content\\\": \\\"Hello, how are you?\\\"\n          }\n        ],\n        \\\"max_tokens\\\": 100\n      }\" \\\n  \"${MODEL_URL}/v1/chat/completions\"\n</code></pre>"},{"location":"user-guide/self-service-model-access/#streaming-chat-completion","title":"Streaming Chat Completion","text":"<p>For streaming responses, add <code>\"stream\": true</code> to the request and use <code>--no-buffer</code> to process the response in real-time:</p> <pre><code>curl -sSk --no-buffer \\\n  -H \"Authorization: Bearer ${ACCESS_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n        \\\"model\\\": \\\"${MODEL_NAME}\\\",\n        \\\"messages\\\": [\n          {\n            \\\"role\\\": \\\"user\\\",\n            \\\"content\\\": \\\"Hello, how are you?\\\"\n          }\n        ],\n        \\\"max_tokens\\\": 100,\n        \\\"stream\\\": true\n      }\" \\\n  \"${MODEL_URL}/v1/chat/completions\"\n</code></pre>"},{"location":"user-guide/self-service-model-access/#understanding-your-access-level","title":"Understanding Your Access Level","text":"<p>Your access is determined by your tier, which controls:</p> <ul> <li>Available models - Which AI models you can use</li> <li>Request limits - How many requests per minute</li> <li>Token limits - Maximum tokens per request</li> <li>Features - Advanced capabilities available</li> </ul>"},{"location":"user-guide/self-service-model-access/#default-tiers","title":"Default Tiers","text":"Tier Requests/min Tokens/min Free 5 100 Premium 20 50,000 Enterprise 50 100,000"},{"location":"user-guide/self-service-model-access/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/self-service-model-access/#common-error-responses","title":"Common Error Responses","text":"<p>401 Unauthorized</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Invalid authentication token\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n</code></pre> <p>403 Forbidden</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Insufficient permissions for this model\",\n    \"type\": \"permission_error\",\n    \"code\": \"access_denied\"\n  }\n}\n</code></pre> <p>429 Too Many Requests</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Rate limit exceeded\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n</code></pre>"},{"location":"user-guide/self-service-model-access/#monitoring-usage","title":"Monitoring Usage","text":"<p>Check your current usage through response headers:</p> <pre><code># Make a request and check headers\ncurl -I -sSk \\\n  -H \"Authorization: Bearer ${ACCESS_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"simulator\", \"messages\": [{\"role\": \"user\", \"content\": \"test\"}]}' \\\n  \"${MODEL_URL}/v1/chat/completions\" | grep -i \"x-ratelimit\"\n</code></pre>"},{"location":"user-guide/self-service-model-access/#common-issues","title":"\u26a0\ufe0f Common Issues","text":""},{"location":"user-guide/self-service-model-access/#authentication-errors","title":"Authentication Errors","text":"<p>Problem: <code>401 Unauthorized</code></p> <p>Solution: Check your token and ensure it's correctly formatted:</p> <pre><code># Correct format\n-H \"Authorization: Bearer YOUR_TOKEN\"\n\n# Wrong format\n-H \"Authorization: YOUR_TOKEN\"\n</code></pre>"},{"location":"user-guide/self-service-model-access/#rate-limit-exceeded","title":"Rate Limit Exceeded","text":"<p>Problem: <code>429 Too Many Requests</code></p> <p>Solution: Wait before making more requests, or contact your administrator to upgrade your tier.</p>"},{"location":"user-guide/self-service-model-access/#model-not-available","title":"Model Not Available","text":"<p>Problem: <code>404 Model Not Found</code></p> <p>Solution: Check which models are available in your tier:</p> <pre><code>curl -X GET \"${MAAS_API_URL}/v1/models\" \\\n  -H \"Authorization: Bearer ${ACCESS_TOKEN}\"\n</code></pre>"}]}